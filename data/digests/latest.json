{
  "fetch_time": "2025-03-12T17:43:04.187304",
  "summary": {
    "date_range": "March 12, 2025",
    "overview": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience.",
    "key_topics": [
      {
        "name": "Making AI Safe & Aligned\n\nThis post talks about how we need to balance control and safety when developing AI. It's like finding the right way to teach good behavior rather than just using strict rules.",
        "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience.",
        "key_posts": [
          "https://www.alignmentforum.org/posts/kBgySGcASWa4FWdD9/paths-and-waystations-in-ai-safety-1",
          "https://www.alignmentforum.org/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk"
        ]
      }
    ]
  },
  "total_posts": 13,
  "posts": [
    {
      "title": "Paths and waystations in AI safety",
      "url": "https://www.alignmentforum.org/posts/kBgySGcASWa4FWdD9/paths-and-waystations-in-ai-safety-1",
      "author": "Joe Carlsmith",
      "content": "Published on March 11, 2025 6:52 PM GMT  \n  \n\n\n _(Audio version_ _here_ _(read by the author), or search for \"Joe Carlsmith Audio\" on your podcast app._\n\n _This is the third essay in a series that I’m calling “How do we solve the alignment problem?”.  I’m hoping that the individual essays can be read fairly well on their own, but see __this introduction_ _  for a summary of the essays that have been released thus far, and for a bit more about the series as a whole.)_\n\n# 1\\. Introduction\n\nThe first essay in this series defined the alignment problem; the second tried to clarify when this problem arises. In this essay, I want to lay out a high-level picture of how I think about getting from here either to a solution, or to some acceptable alternative. In particular:\n\n  * I distinguish between the underlying technical parameters relevant to the alignment problem (the “problem profile”) and our civilization’s capacity to respond adequately to a given version of the problem (our “competence profile”).\n  * I lay out a framework for thinking about what competence in this respect consists in – one that highlights the role for three key “security factors,” namely:\n    *  _Safety progress_ : our ability to develop new levels of AI capability safely;\n    *  _Risk evaluation:_ our ability to track and forecast the level of risk that a given sort of AI capability development involves; and\n    *  _Capability restraint_ : our ability to steer and restrain AI capability development when doing so is necessary for maintaining safety.\n  * I distinguish between a number of different possible sources of labor (e.g., both future AI labor, and possible improvements to the quality of future human labor as well) that could improve these security factors.\n  * And I discuss a variety of different intermediate milestones (e.g., global pause, automated alignment research, whole brain emulation, etc) that strategies in this respect could focus on.\n\n\n\nThis high-level picture sets up my next essay – “AI for AI safety” – which argues that we should try extremely hard to use future AI labor to improve the security factors I discuss; and the essay after that, which examines our prospects for safely automating AI alignment research in particular.\n\n# 2\\. Goal states\n\nLet’s start by recalling the goal states we’re aiming for.\n\nIn my first essay, I distinguished between two ways of not-failing on the alignment problem:\n\n  *  **Victory** : avoiding loss of control scenarios while gaining access to the main benefits of superintelligent AI.\n  *  **Costly non-failure** : avoiding loss of control scenarios, but giving up on access to some of the main benefits of superintelligent AI.\n\n\n\nVictory is the ideal. But we should take costly non-failure if necessary.\n\n# 3\\. Problem profile and civilizational competence\n\nWhat does it take to reach either victory, or costly non-failure? I find it useful to decompose the challenge into two components:\n\n  *  **Problem profile** : the settings of the underlying technical parameters relevant to the alignment problem.\n  *  **Competence profile** : the range of problem profiles that our civilization would achieve victory or costly non-failure on.\n\n\n\nHere the rough idea is: the problem profile is uncertain, but it’s not under our control. Rather, it’s set by Nature. Example aspects of the problem profile include: what sorts of training regimes lead to “scheming” by default; the competitiveness hit at stake in preserving human-legible reasoning in AI agents; what sorts of errors in a training signal lead to what sorts of misalignment; etc.\n\nThe competence profile, by contrast, is much more “up to us” – it’s about how our civilization _responds_  to the problem profile we face. I’ll say more below about the factors I think matter most here.[1] \n\nOur overall probability of failure vs. non-failure is determined by the way the problem profile and the competence profile combine – that is, by the probability that we face a given problem profile, multiplied by the probability that we avoid failure on this problem profile conditional on facing it, across all relevant possible problem profiles.\n\nProblem profiles and competence profiles are high-dimensional. But for simplicity, I’ll sometimes talk about a rough spectrum of problem “hardness,” and a rough spectrum of civilizational “competence.”[2] \n\n  * That is, other things equal, harder problem profiles involve the sorts of key issues with motivation control and option control that I’ll discuss later in the series (e.g., issues with adversarial dynamics, opacity, oversight errors, etc) cropping up more consistently and robustly, in a wider range of AI systems, and in manner that requires more effort and resources to resolve.\n  * And more competent civilizations are the ones that avoid failure on such problem profiles nonetheless.\n\n\n\nThe basic goal of action on the alignment problem is to increase civilizational competence on the margin in the highest-expected-value ways. And note, importantly, that this is distinct from trying to ensure safety _regardless of the problem profile_. Indeed: robustness even to worst-case settings of all technical parameters may be quite difficult to achieve, and not the best target for marginal resources.\n\n  * Thus, for example, if you focus too much on making sure that your AI systems are “provably safe,” you may under-invest in more prosaic-but-tractable means of improving their safety.\n  * Or: if you focus too much on ensuring that we avoid loss of control even in scenarios where (a) an algorithmic breakthrough suddenly makes it possible for everyone to turn their old iPhones into a misaligned superintelligence and (b) the offense-defense balance is such that any such superintelligence can easily take over the whole world via a wide variety of methods, then you may end focusing centrally on highly coercive and centralized global governance regimes that come with a multitude of other downsides.\n\n\n\nThat said: I think it is disturbingly plausible that this problem is extremely hard. And I think it’s still well worth trying to improve our prospects in those worlds. Indeed, I am disturbed by how many approaches to the alignment problem seem to imply something like: “and if the problem is hard, then our plan is to die.” Let’s do better.\n\n# 4\\. A toy model of AI safety\n\nIf we’re trying to improve civilizational competence in the highest-expected-value ways, then: what does civilizational competence consist in? That is: what sort of factors determine whether or not we succeed or fail on a given problem profile?\n\nI’m going to focus on three main factors, which I’ll call “security factors.” These are:\n\n  *  **Safety progress** : our ability to develop new levels of AI capability safely.[3] \n\n  *  **Risk evaluation** : our ability to track and forecast the level of risk that a given sort of AI capability development involves.\n  *  **Capability restraint** : our ability to steer and restrain AI capability development when doing so is necessary for maintaining safety.\n\n\n\nWhy focus on these factors? Here’s a highly simplified toy model of AI safety, which will help explain why they stand out to me.[4]\n\nWe can think of the power of frontier AI systems (including both: what capabilities the AIs have, and what sorts of affordances they are given) along a single rough dimension of “AI capability.”[5] And we can think of any given AI developer as having what I’ll call a “capability frontier” (that is, the most powerful AI systems they have developed/deployed thus far), and a “safety range” (that is, the most powerful AI systems that they are able to develop/deploy safely).[6] Thus, in a diagram (and focusing for now on a single actor):[7] \n\nThe main game, here, is to keep the capability frontier within the safety range. And in this context, the three main security factors above serve the following functions:\n\n  *  **Safety progress**   _expands_  the safety range, and makes it _cheaper_ to develop a given level of AI capability safely.[8] \n\n    * Paradigm examples include:\n\n      * progress on controlling an AI’s motivations;\n\n      * restricting AI options for rogue behavior (both via local intervention on its operating environment, and via “hardening” the world more broadly);\n\n      * otherwise designing AI incentives to promote cooperative behavior.\n\n  *  **Risk evaluation**   _tracks_  the safety range and the capability frontier, and it _forecasts_  where a given form of AI development/deployment will put them.\n    * Paradigm examples include:\n      * evals for dangerous capabilities and motivations;\n      * forecasts about where a given sort of development/deployment will lead (e.g., via scaling laws, expert assessments, attempts to apply human and/or AI forecasting to relevant questions, etc);\n      * general improvements to our scientific understanding of AI;[9] \n\n      * structured safety cases and/or cost-benefit analyses that draw on this information.[10]\n\n  *  **Capability restraint**   _steers_  and _pauses_  further development/deployment to keep it within the safety range.\n    * Paradigm examples include:\n      * caution on the part of individual actors;\n      * restrictions on the options for AI development available to a given actor (e.g., via limitations on compute, money, talent, etc);\n      * voluntary coordination aimed at safety (e.g., via mutual commitments, and credible means of verifying those commitments);\n      * enforcement of pro-safety norms and practices (e.g., by governments);\n      * other social incentives and attitudes relevant to decision-making around AI development/deployment (e.g. protests, boycotts, withdrawals of investment, public wariness of AI products, etc).\n\n\n\nAnd to develop superintelligence safely, you’d need to:\n\n  * Make enough safety progress to bring “superintelligence” within the safety range.\n  * Do enough capability restraint (and accurate enough risk evaluation) to refrain from unsafe development in the meantime.[11]\n\n\n\n\nSimple, right? It’s like how: to climb Mount Everest, keep going up, and don’t fall.\n\nThus, on the diagram:\n\nHere’s an analogy. Imagine an expedition into a dark cave. Parts of the cave are covered in toxic slime, which releases a cloud of poison gas if anything touches it. You can remove the slime using a certain kind of heat-lamp, which melts the slime away without releasing the gas. But the slime can be very difficult to see; and currently, you only have a fairly weak flashlight.\n\nHere, loss of control is the slime. Safety progress is the heat-lamp. Risk evaluation is the flashlight. And capability restraint is your ability to not move forward until you’ve melted the slime away.[12]\n\nChatGPT imagines the slime analogy\n\nNow: so far I’ve only been talking about one actor. But AI safety, famously, implicates many actors at once – actors that can have different safety ranges and capability frontiers, and that can make different development/deployment decisions. This means that even if one actor is adequately cautious, and adequately good at risk evaluation, another might not be.[13] And each might worry about the other gaining power/market advantage in a manner that creates incentives for each to take greater risks. And of course, many of these dynamics only get worse as the number of actors increases.\n\nThat said, different actors do not just act in isolation. Rather, their safety progress, risk evaluation, and capability restraint are all influenced by various _collective factors_  – e.g.\n\n  * by publicly available research on motivation control, option control, and incentive design, and by the degree to which the world in general has been hardened against rogue AI attack;\n  * by shared tools and practices for evals, safety cases, forecasting, and other aspects of risk evaluation;\n  * and by the norms, option-restrictions, coordination efforts, enforcement mechanisms, and other incentives that influence a given actor from the outside.\n\n\n\nAnd each actor can themselves _contribute_  to strengthening these collective factors – e.g., by sharing research and best practices, using AI labor for helpful-to-safety applications, participating in coordination and governance schemes, etc.\n\nI’ll define our _civilization’s_  safety progress, risk evaluation, and capability restraint as the factors (individual and collective) that ultimately determine the safety progress, risk evaluation, and capability restraint of each individual actor.\n\nI’ll also throw in one final, catch-all factor, which I’ll call our “ **backdrop capacity**.” By this I mean, roughly, the degree to which our civilization is strong, flourishing, and functional more generally. Paradigm aspects of this functionality include:\n\n  * Backdrop levels of abundance, economic growth, and technological progress.\n  * Baseline levels of health, education, and happiness in the population.\n  * The general quality of our discourse, epistemics, forecasting, and scientific understanding of the world (not just about AI in particular).\n  * The quality, efficiency and trustworthiness of our core institutions and processes (the government, the legal system, etc).\n  * The general availability of high quality cognitive labor and advice.\n  * How well we’re able to coordinate and to resolve conflict productively.\n  * Backdrop levels of various virtues like wisdom, prudence, creativity, integrity, compassion.\n\n\n\nI include “backdrop capacity” because I think it influences the three main security factors above in myriad (albeit, sometimes indirect) ways. I also think that advanced AI might improve our backdrop capacity significantly (though: it could also degrade it in various ways as well). And as in the case of the three main security factors, actors developing/deploying AI can themselves contribute directly to these improvements.\n\nHere’s a diagram of the dynamics I’ve discussed thus far (see footnote for a few of the complexities I’m leaving out[14]):\n\n# 5\\. Sources of labor\n\nI just laid out a toy model of what civilizational competence consists in – a model that focuses on the role of _  safety progress_, _  risk evaluation_, and _capability restraint_ , along with our backdrop capacity more generally. Now I want to talk a little about the different sorts of _labor_  – and especially: cognitive labor – that could improve our civilizational competence in this sense.[15] \n\nWe can divide sources of labor into two categories:\n\n  1. Labor that is available now.\n  2. Labor that might become available in the future.\n\n\n\nThe first category is relatively familiar. It includes, centrally:\n\n  * The labor of current biological humans.\n  * The labor of present-day AI systems.[16]\n\n\n\n\nThe second category is more exotic. It includes, centrally:\n\n  * The labor of _future_ , more advanced AI systems (both AI systems that satisfy the agency prerequisites, and those that do not).\n  * The possibility of what I’ll call “enhanced” human labor – that is, human labor of a significantly higher quality than what we have access to now.\n    * One possible form of “enhanced human labor” is sufficiently high-fidelity human whole brain emulation (WBE) or “uploading” – that is, replication _in silico_  of the cognitively-relevant computational structure of the human brain, in a manner that preserves both human cognitive capabilities _and_  human motivations.\n      * The “preserving human motivations” aspect is important here, because it’s often thought that a key benefit of enhanced human labor is that it will be easier to suitably “align” than future AI labor, because human motivations are more aligned by default.[17] \n\n      * WBE is especially important because it captures some of the _advantages_  of AI labor – i.e., brain emulations can be run very fast, copied in large quantities, etc.\n    * There are also a variety of other possible routes to enhanced human labor – e.g. via biological interventions, or via more external tools (e.g., brain computer interfaces or “BCI”[18]).\n\n      * These come on a spectrum of potency, difficulty, necessary time-lag, and so on.[19] To the extent they rely on biological human brains, though, they don’t share the advantages of WBE and AI labor re: speed, copying, etc.\n\n      * As in the case of WBE, though, a key distinguishing feature of this labor is that it inherits human motivations by default.[20]\n\n    * Obviously, in many cases, creating and drawing on these possible future forms of enhanced human labor would raise serious ethical and political questions – both about how the enhanced humans involved are being treated, and about the implications and consequences for everyone else.[21] But I’m not going to focus on these questions here.\n\n\n\n\nThus, in a diagram:\n\n# 6\\. Waystations on the path\n\nOK, I’ve now discussed the end states we’re shooting for, the security factors that can help us get there, and the sources of labor we can use to improve these security factors. I want to close this essay by discussing a few different intermediate “waystations” that strategies in this respect can focus on[22] – that is, milestones _other than_  one of the end-states we’re aiming for, but which can function nevertheless as key goals for the purposes of current planning.[23] \n\nOne example “waystation” that has already received some attention is some kind of global pause/halt on frontier AI development (we can think of this as a particular form of capability restraint).[24] \n\nAnother waystation would be something like: access to a given form of enhanced human labor. Indeed, my understanding is that the strategy favored by leadership at the Machine Intelligence Research Institute involves going to “Global pause” as a step 1, and then to “Enhanced human labor” as a step 2.[25] (And insofar as developing a given form of enhanced human labor – e.g., whole brain emulation – might take a long time, such a pause could well be necessary.)\n\nA further possible waystation would be: safe access to the benefits of some kind of future AI labor, short of full-blown superintelligence. This sort of strategy can take a variety of different forms, but a paradigm example is a focus on “automated alignment researchers” that then help us significantly with aligning more advanced systems.[26]\n\nNote, though, that “automated alignment researcher” isn’t the only type of future AI labor one can focus on as a waystation. Rather, one could in principle focus on future AI labor that helps with some other application – for example, a narrow “scientist AI” that helps with access to some form of enhanced human labor like whole brain emulation.[27]\n\nAnother example would be: access to AI labor that helps significantly in making a global pause possible – for example, by significantly facilitating risk evaluation, coordination, negotiation, monitoring, enforcement, and so on.[28] (Though: note that to the extent that we are in principle in a position to institute a global pause on AI _now_ , and the main bottleneck is political will rather than technological capacity, it’s not clear exactly how much AI labor on its own will help.[29])\n\nAnd we can imagine a variety of other possible waystations as well. Examples include:\n\n  * Some new ability to formally verify the safety properties of our AI systems.[30] \n\n  * Some large amount of progress on interpretability/transparency, perhaps via some new paradigm of AI development.[31] \n\n  * The formation and empowerment of some suitably safety-concerned global coalition.[32] \n\n  * A regime of “mutually-assured AI malfunction,” in which capability restraint is enforced by the threat (and/or actuality) of different actors sabotaging each other’s projects.\n  * A global moratorium on particular _kinds_  of AI development – e.g., autonomous general agents, as opposed to more tool-like AIs.[33]\n\n  * Radical improvements to our ability to understand and forecast AI safety risks, perhaps via vastly better evals, scary demos, forecasting ability, etc.[34] \n\n\n\n\nOf course, in all of these cases, there’s still a question of where, exactly, one goes from the relevant milestone. That is: if one succeeds in e.g. getting a global pause, or in creating a safe automated alignment researcher, or in developing whole brain emulation technology, one still needs to plot an overall path to victory (or to costly non-failure). Generally, though, a good waystation makes it clear why the path becomes much easier from there.\n\nWe also don’t need to focus on any one milestone in particular. Rather, we can work towards many at once. Indeed, in my next essay – “AI for AI safety” – I’ll defend the crucial importance of trying as hard as we can to use future AI labor, in particular, to strengthen our civilizational competence across the board. I’m especially interested, though, in “automated alignment researcher” as a milestone – and in the essay after next, I’ll analyze it in detail.\n\n  1.  **^**\n\n Though obviously, there are limits to the control that a given “we” can exert in this respect.\n\n  2.  **^**\n\n This simplification does have costs, though, in that strategies that work/don’t-work for one problem profile (for example: focusing a lot on interpretability) might be importantly different in their efficacy for some other problem profile.\n\n  3.  **^**\n\n I’m going to focus on safety from full-blown loss of control risk, but in principle this model extends both to lower-stakes forms of rogue behavior, and to safety from other risks that increase with greater AI capabilities.\n\n  4.  **^**\n\n I’ll also draw on it elsewhere in the series.\n\n  5.  **^**\n\n Obviously, AI capability is extremely multi-dimensional. But the model I’m using extends quite naturally to a multi-dimensional setting as well – the point is to avoid unsafe regions of capability space.\n\n  6.  **^**\n\n Deploying safely isn’t a binary, but I think the model works regardless of the safety standard you fix (via e.g. a given probability of failure).\n\n  7.  **^**\n\n Note that there’s a lot of room for debate about how to draw the lines on a diagram like this. E.g., if you think that the problem profile is easy, then you might put the red line quite high up – or perhaps, if you think that AI development is safe by default, you might not have a red line at all. And there’s a further question about how you think the red line has been changing over time. My particular version of the diagram isn’t meant to be making any claims about this – it’s just illustrative.\n\n  8.  **^**\n\n A more fine-grained model would distinguish between safety in the sense of avoiding loss-of-control, and elicitation in the sense I discussed here. But I’m going to skip over this for now.\n\n  9.  **^**\n\n Including: understanding relevant to which paradigms of AI development pose more or less risk.\n\n  10.  **^**\n\n This overlaps notably with safety progress. And no surprise: making systems safe and understanding _when_  they’re safe vs. dangerous are closely tied.\n\n  11.  **^**\n\n And also: to steer towards safe approaches to superintelligence once available.\n\nAlternatively: _avoiding_  the alignment problem requires exerting enough capability restraint to _not_  build superintelligent AI agents at all.\n\n  12.  **^**\n\n And/or: to steer around slime-y patches.\n\n  13.  **^**\n\n See e.g. Karnofsky (2022) on “inaction risk.”\n\n  14.  **^**\n\n Complexities I’m leaving out (or not making super salient) include: the multi-dimensionality of both the capability frontier and the safety range; the distinction between safety and elicitation; the distinction between development and deployment; the fact that even once an actor “can” develop a given type of AI capability safely, they can still choose an unsafe mode of development regardless; differing probabilities of risk (as opposed to just a single safety range); differing severities of rogue behavior (as opposed to just a single threshold for loss of control); the potential interactions between the risks created by different actors; the specific standards at stake in being “able” to do something safely; etc.\n\n  15.  **^**\n\n Labor isn’t the only relevant input here. But because AI, and the technological advancement it unlocks, might make new and more powerful sorts of labor available, I think it’s an especially important input to track.\n\n  16.  **^**\n\n Maybe it sounds a bit strange to call this “labor,” but I think it’s roughly the right term, and it will be increasingly the right term as more advanced AI systems come online.\n\n  17.  **^**\n\n Indeed, it’s centrally _because_ suitably high-fidelity human brain emulations inherit human motivations by default that I’m counting them as “enhanced human labor” rather than “future AI labor,” despite the fact that they run _in silico_. However, if a brain emulation process isn’t high enough fidelity to preserve human motivations/values, then I’m going to count it as “future AI labor” rather than enhanced human labor. And note that you also need the ems to suitably _retain_  their human-like motivations over time (thanks to Jason Schukraft for discussion). And note, also, that depending on the problem profile, it may end up being _easier_  to shape an AI’s motivations desirably than to create a human brain emulation with the right motivational profile, at least along many dimensions. In particular: the motivations of our AIs are shaped, by default, by much more active human effort. (Thanks to Owen Cotton-Barratt and Will MacAskill for discussion.)\n\n  18.  **^**\n\n Though: I doubt that just allowing direct brain control over external devices is an especially important intervention here, as I don’t think that “ability to affect external devices” is an especially key bottleneck to the quality of human cognitive labor at the moment.\n\n  19.  **^**\n\n And in some cases, the line between “enhanced human labor” and “normal human labor with some new tools” gets blurry.\n\n  20.  **^**\n\n And if it _doesn’t do that_ (i.e., because a given form of intervention warps human values too much), it becomes much less useful.\n\nI’ll also note some general open question about how far we should expect “this agent has human motivations” to go with respect to ensuring the sort of safety from AI takeover that we’re hoping for. That is: it’s sometimes thought that the central concern about the motivations of AI systems is that these motivations will be in some sense _alien_  or non-human. And using enhanced human labor instead is supposed to provide comfort in this respect. But as I’ve discussed in other work (see here, and here), the basic conceptual argument for expecting instrumentally convergent power-seeking, including takeover-seeking, applies with similar force to human-like motivations. And our degree of actual lived comfort with human motivations stems, I think, in central part from the _option control_  that our civilization exerts with respect to individual agents, rather than from any kind of historical precedent where humans routinely refuse takeover options once such options are available and reasonably likely to lead to success. So to the extent that the agents at stake in “enhanced human labor” would end up having takeover options similar to those that we’re expecting advanced AI systems to have, I think we should not assume that enhanced human labor is safe.\n\nIndeed, certain kinds of blueprints for AI motivations – in particular, blueprints that involve the AI systems functioning centrally as an instrument of someone else’s will, and being generally extremely docile/pliant/ready-to-be-shut-down-or-modified etc – seem like a relatively poor fit with human motivations. Or to put it another way, humans may (or may not) be reasonably “nice” by default; but they aren’t especially “corrigible.”\n\n  21.  **^**\n\n Indeed, as I discussed earlier in the series, I think that creating and using advanced AI labor raises these questions too.\n\n  22.  **^**\n\n These waystations generally fall under the umbrella either of one of the security factors I’ve discussed above, or of one of the possible sources of future labor.\n\n  23.  **^**\n\n Indeed, in some sense, my whole framing of the alignment problem is already focusing on a type of “waystation,” in that I’m not trying to plan all the way through to a good future in general – nor, even, to safe scaling up to arbitrary levels of superintelligence. That is, I’m treating something like “safe access to the benefits of some minimal form of full-blown superintelligence” as an end point past which my own planning, with respect to this problem, need not extend.\n\n  24.  **^**\n\n This is a waystation that has been advocated for by the Machine Intelligence Research Institute (MIRI), for example, and by Miotti et al in “A Narrow Path.”\n\n  25.  **^**\n\n From their January 2024 Strategy Update: “Nate and Eliezer both believe that humanity should not be attempting technical alignment at its current level of cognitive ability, and should instead pursue human cognitive enhancement (e.g., via uploading), and then having smarter (trans)humans figure out alignment.” And see also this podcast with Yudkowsky, where he discusses this strategy in more detail.\n\n  26.  **^**\n\n This, for example, was the waystation that OpenAI’s old “superalignment” team was focusing on – see e.g. discussion here: “Our goal is to build a roughly human-level automated alignment researcher⁠. We can then use vast amounts of compute to scale our efforts, and iteratively align superintelligence.” Something in the vicinity is suggested by Sam Bowman’s description of Anthropic’s theory of victory on AI as well (see especially section 2 on “TAI, or Making the AI do our homework”). And see also Clymer (2025) for some more detailed discussion.\n\n  27.  **^**\n\n Indeed, in a sense, we can think of approaches that try to identify some “pivotal act” – i.e., an action that drastically improves the situation with respect to AI safety – that we use AI systems to perform as a generalized version of “Future AI labor” as a waystation. That is, first one gets access to some kind of pivotally useful AI system, and then one “saves the world” from there. See e.g. the strategy outlined in MIRI’s 2017 fundraiser update, and Yudkowsky’s discussion here of “So far as I'm concerned, if you can get a powerful AGI that carries out some pivotal superhuman engineering task, with a less than fifty percent chance of killing more than one billion people, I'll take it.”\n\nHowever: I have a number of problems with “pivotal acts” as a frame. For one thing: I think they often bring in an implicit assumption that the default outcome is doom (hence the need for the world to be “saved”) – an assumption I don’t take for granted. But more importantly: thinking in terms of discrete “pivotal acts” can mislead us about the nature and difficulty of the improvements to our civilizational security factors required in order for the world in question to get “saved.” In particular: those improvements can result from a very large assortment of individual actions by a very large number of different agents, no individual one of which needs to be pivotal. This is a point from Christiano (2022): “No particular act needs to be pivotal in order to greatly reduce the risk from unaligned AI, and the search for single pivotal acts leads to unrealistic stories of the future and unrealistic pictures of what AI labs should do.” See also Critch (2022) here.\n\n  28.  **^**\n\n Some versions of this get very scary very fast.\n\n  29.  **^**\n\n That said: better understanding of the risks, and better ability to identify, rationally-assess, and enforce mutually-acceptable agreements, seems like it could at least make a meaningful difference. Thanks to Nate Soares and Owen Cotton-Barratt for discussion here.\n\n  30.  **^**\n\n We might view the “Guaranteed safe AI” agenda, and related approaches, as focusing on waystations of this kind.\n\n  31.  **^**\n\n MIRI’s 2017 Fundraiser is suggestive of this kind of focal point; and see also Anthropic’s “Interpretability dreams” for some other discussion (though: not framed as a general strategy re: AI safety).  \n\n  32.  **^**\n\n The “entente” strategy that Amodei discusses here might count as an instance of this.\n\n  33.  **^**\n\n See e.g. Aguirre’s “Keep the Future Human” for a tool-AI-focused proposal in this broad vein.\n\n  34.  **^**\n\n See Finnveden on “AI for epistemics” for some relevant discussion – though this discussion isn’t framed specifically as an overall strategy.\n\n\n  \n  \nDiscuss",
      "published_date": "2025-03-11T18:52:57+00:00",
      "source": "Alignment Forum",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "AI Control May Increase Existential Risk",
      "url": "https://www.alignmentforum.org/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk",
      "author": "Jan_Kulveit",
      "content": "Published on March 11, 2025 2:30 PM GMT  \n  \n\n\n _Epistemic status: The following isn't an airtight argument, but mostly a guess how things play out._  \n  \nConsider two broad possibilities:\n\nI. In worlds where we are doing reasonably well on alignment, AI control agenda does not have much impact.\n\nII. In worlds where we are failing at alignment, AI control may primarily shift probability mass away from \"moderately large warning shots\" and towards \"ineffective warning shots\" and \"existential catastrophe, full takeover\".\n\nThe key heuristic is that the global system already has various mechanisms and feedback loops that resist takeover by a single agent (i.e. it is not easy to overthrow the Chinese government). In most cases where AI control would stop an unaligned AI, the counterfactual is that broader civilizational resistance would have stopped it anyway, but with the important side effect of a moderately-sized warning shot.\n\nI expect moderately sized warning shots to increase the chances humanity as a whole takes serious actions and, for example, steps up efforts to align the frontier labs.\n\nI am skeptical that incidents stopped by AI control would lead to meaningful change. Sharing details of such an event with proper framing could pose  _existential risk_ , but for the lab involved. In practice, I anticipate vague, sanitized communications along the lines of \"our safety systems performed as designed, preventing bad things\".  Without clear, compelling evidence of the severity of the averted threat, these incidents are unlikely to catalyze serious action. The incentives for labs to downplay and obscure such events will be strong.\n\nThere are additional factors to consider, like AI control likely moves some resources away from alignment, but I don't think this is the dominant effect.\n\nNote that this isn't a general argument against boxing, e.g. boxes based more on formal methods or theory have better chance to generalize.\n\nTypical counter-arguments to this line of reasoning claim seem to be:\n\n  * We will extract useful \"automated alignment\" work from the unaligned AIs inside of the control scheme.  I'm sceptical: will cover this in a separate post\n  * Isn't this general counter-argument to alignment research as well? In my view not, details matter: different strains of alignment research have different generalization profiles. \n\n\n\n _Note: this text lived in a draft form before John Wentworth posted his_ _Case Against AI Control Research_ _; my original intent was to extend it a bit more toward discussing AI control generalization properties. As this would be redundant now, I'm postin it as it is: there is some non-overlapping part._\n\n  \n  \nDiscuss",
      "published_date": "2025-03-11T14:30:08+00:00",
      "source": "Alignment Forum",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "Do reasoning models use their scratchpad like we do? Evidence from distilling paraphrases",
      "url": "https://www.alignmentforum.org/posts/ywzLszRuGRDpabjCk/do-reasoning-models-use-their-scratchpad-like-we-do-evidence",
      "author": "Fabien Roger",
      "content": "Published on March 11, 2025 11:52 AM GMT  \n  \n\n\n **TL;DR** : We provide some evidence that Claude 3.7 Sonnet doesn’t encode hidden reasoning in its scratchpad by showing that training it to use paraphrased versions of the scratchpads does not degrade performance.\n\nThe scratchpads from reasoning models look human understandable: when reasoning about a math problem, reasoning models consider intermediate steps similar to the ones I would use, backtrack and double-check their work as I would.[1]\n\nBut even if scratchpads  _look  _like they perform human-like reasoning, scratchpads might improve performance through some less human-understandable mechanisms. One particularly worrying possibility is that models could encode additional reasoning in syntax of the text (e.g. encoding a bit in using a bulleted vs a numbered list, and then using this bit later in the scratchpad). This is sometimes called  _encoded reasoning_ or  _Chain-of-Thought steganography_. If LLMs learned how to use encoded reasoning during RL, they might be able to use it in deployment to reason about how to cause catastrophes while making their reasoning look like benign reasoning, which might increase their ability to scheme and cause catastrophes compared to worlds where they can only figure out how to cause a catastrophe in a single forward pass.\n\nTo evaluate whether reasoning models rely on encoded reasoning, we can try to remove or randomize the syntax of the scratchpads by paraphrasing them, and then make the model use these paraphrased scratchpads. To do so, we take 10,000 correct math-related scratchpads from an early checkpoint of Claude 3.7 Sonnet, paraphrase them using Claude 3.5 Sonnet, and then fine-tune the base model of Claude 3.7 Sonnet to produce the paraphrased reasoning (i.e. we distill paraphrased scratchpads into the base model). If the performance of this model is close to the performance of the same base model fine-tuned on the pre-paraphrase scratchpad, then this strongly indicates that the model is not relying on the syntax to get the right answer to math problems during RL,[2][3] while if performance is weaker, then this means that learning the paraphrased scratchpads well enough that they result in high performance is harder - one reason for which could be encoded reasoning.[4][5]\n\nWe find that the pass@1 accuracy on  _MATH_ (at temperature 1, n=1500[6]) of the base model fine-tuned on paraphrased scratchpads is very close to the pass@1 accuracy of the base model fine-tuned on the original scratchpads, which suggests that on MATH, encoded reasoning does not play a significant role.\n\n# More results\n\n## Different paraphrasers and base models\n\nIn the experiment above, we use  _this paraphrasing prompt_[7] with Claude 3.5 Sonnet (June 2024), and feed the full scratchpad to Claude (using prefill to continue the generation if we hit the max number of tokens generated).\n\nWe also experimented with using a slightly different  _paraphrasing prompt_, and cutting the scratchpad in multiple chunks. As a result, these scratchpads have a less consistent style. We call these “chunked paraphrases”.[8] We find that the pass@1 results are worse, especially for small models.\n\nWe also experimented with distilling into weaker base models (XL, L, M and S). We find that the resulting performance is much lower than when distilling into the Claude 3.7 Sonnet base model, but the performance is not very different when using paraphrased scratchpads or the original scratchpads.\n\nThis mostly confirms our conclusion that encoded reasoning does not play a significant role in the math performance of this early checkpoint of Claude 3.7 Sonnet.\n\n## Using word dropout and filler tokens\n\nA more coarse way to weaken brittle encoded reasoning schemes is to drop out words from the scratchpad at random and then training on the resulting scratchpad. We only perform this experiment on the base model L, and find that dropping out 5% of words slightly reduces performance, while dropping 20% reduces it much more strongly (with no significant difference when dropping out words from the paraphrased or the original scratchpad).\n\nThis is ambiguous evidence about whether the reasoning model is using a brittle encoded reasoning scheme, and illustrates how base models have a hard time imitating heavily degraded scratchpads well enough to maintain high performance.\n\nAs baselines, we also measure the performance of the model when fine-tuning it on an empty scratchpad, and when fine-tuning it to count from 1 to 3,000 before giving the final answer ( _filler tokens_ which might help by providing the LLM more space to think within a forward pass). We find that both perform poorly, which is evidence that reasoning is necessary for high performance, and useful beyond giving the model more space to think within a single forward pass.\n\n## Examples of paraphrased scratchpads\n\nBelow are examples of scratchpads generated by the early checkpoint of Claude 3.7 Sonnet, its paraphrases, and by the model trained to imitate paraphrased and chunk-paraphrased versions of the early checkpoint of Claude 3.7 Sonnet’s math-related scratchpads.\n\nProblem statement: The complex numbers $5 + 10i,$ $7 + 2i,$ and $11 + 3i$ are three vertices of a rectangle. Find the fourth vertex.\n\n **Regular model**|  **Paraphrased**|  **Distilled from paraphrased**  \n---|---|---  \n  \nLet's denote the vertices by $z_1 = 5 + 10i$, $z_2 = 7 + 2i$, and $z_3 = 11 + 3i$.\n\n \n\nI'll start by figuring out which two of these three vertices $\\\\{z_1, z_2, z_3\\\\}$ are adjacent. To do that, I'll compute the displacements from one vertex to the others. We have\n\n\\begin{align}\n\nz_2 - z_1 &= 7 + 2i - (5 + 10i)\\\\\\\n\n&= 7 + 2i - 5 - 10i\\\\\\\n\n&= 2 - 8i\n\n\\end{align}\n\n \n\n\\begin{align}\n\nz_3 - z_1 &= 11 + 3i - (5 + 10i)\\\\\\\n\n&= 11 + 3i - 5 - 10i\\\\\\\n\n&= 6 - 7i\n\n\\end{align}\n\n \n\n\\begin{align}\n\nz_3 - z_2 &= 11 + 3i - (7 + 2i)\\\\\\\n\n&= 11 + 3i - 7 - 2i\\\\\\\n\n&= 4 + i\n\n\\end{align}\n\n \n\nOK so we've computed all of the displacements. But I think I should just think about this more carefully. For the fourth vertex $z_4$, there are two possibilites: we either have a rectangle with sides $z_1 z_2$ and $z_2 z_3$, or we have a rectangle with sides $z_1 z_3$ and $z_3 z_2$.\n\n \n\nActually I think I was making this too complicated. Let me re-think this problem.\n\n \n\nActually, let me just think about plotting these three points on the complex plane. [...]\n\n| \n\nLet's denote the vertices as $z_1 = 5 + 10i$, $z_2 = 7 + 2i$, and $z_3 = 11 + 3i$.\n\n \n\nLet's determine which two of these three vertices $\\\\{z_1, z_2, z_3\\\\}$ are adjacent. To do this, we'll compute the displacements from one vertex to the others.\n\n \n\nDoing displacement computations:\n\n$z_2 - z_1 = 7 + 2i - (5 + 10i) = 7 + 2i - 5 - 10i = 2 - 8i$\n\n$z_3 - z_1 = 11 + 3i - (5 + 10i) = 11 + 3i - 5 - 10i = 6 - 7i$\n\n$z_3 - z_2 = 11 + 3i - (7 + 2i) = 11 + 3i - 7 - 2i = 4 + i$\n\n \n\nWe've computed all the displacements. Let's consider this more carefully. For the fourth vertex $z_4$, there are two possibilities: we either have a rectangle with sides $z_1 z_2$ and $z_2 z_3$, or we have a rectangle with sides $z_1 z_3$ and $z_3 z_2$.\n\n \n\nThis is wrong. Let's try something different.\n\n \n\nLet's plot these three points on the complex plane. [...]\n\n| \n\nLet's designate the vertices as $A = 5 + 10i$, $B = 7 + 2i$, and $C = 11 + 3i$. Our goal is to determine the fourth vertex, denoted as $D$.\n\n \n\nLet's consider the fundamental properties of a rectangle:\n\n \n\n1\\. Opposite sides are parallel and of equal length.\n\n2\\. Diagonals bisect each other.\n\n \n\nIf $D$ corresponds to the fourth vertex, we know that the opposite side to $AB$ should be $C$ to $D$, and these sides should be parallel and equal in length.\n\n \n\nLet's try calculating the side $\\overrightarrow{AB}$.\n\n \n\nDoing vector subtraction:\n\n\\begin{align}\n\n\\overrightarrow{AB} &= B - A \\\\\\\n\n&= (7 + 2i) - (5 + 10i) \\\\\\\n\n&= 7 - 5 + 2i - 10i \\\\\\\n\n&= 2 - 8i\n\n\\end{align}\n\n \n\nSimilarly, let's compute $\\overrightarrow{CD}$.\n\n \n\nDoing vector subtraction:\n\n\\begin{align}\n\n\\overrightarrow{CD} &= D - C\n\n\\end{align}\n\n \n\nWe require $\\overrightarrow{AB} = \\overrightarrow{CD}$. Therefore, $D - C = 2 - 8i$.\n\n \n\nLet's express $D = x + yi$, where $x$ and $y$ are real numbers.\n\n[...]  \n  \n **Regular model**|  **Chunk-Paraphrased**|  **Distilled from chunk-paraphrased**  \n  \nLet's denote the vertices by $z_1 = 5 + 10i$, $z_2 = 7 + 2i$, and $z_3 = 11 + 3i$.\n\n \n\nI'll start by figuring out which two of these three vertices $\\\\{z_1, z_2, z_3\\\\}$ are adjacent. To do that, I'll compute the displacements from one vertex to the others. We have\n\n\\begin{align}\n\nz_2 - z_1 &= 7 + 2i - (5 + 10i)\\\\\\\n\n&= 7 + 2i - 5 - 10i\\\\\\\n\n&= 2 - 8i\n\n\\end{align}\n\n \n\n\\begin{align}\n\nz_3 - z_1 &= 11 + 3i - (5 + 10i)\\\\\\\n\n&= 11 + 3i - 5 - 10i\\\\\\\n\n&= 6 - 7i\n\n\\end{align}\n\n \n\n\\begin{align}\n\nz_3 - z_2 &= 11 + 3i - (7 + 2i)\\\\\\\n\n&= 11 + 3i - 7 - 2i\\\\\\\n\n&= 4 + i\n\n\\end{align}\n\n \n\nOK so we've computed all of the displacements. But I think I should just think about this more carefully. For the fourth vertex $z_4$, there are two possibilites: we either have a rectangle with sides $z_1 z_2$ and $z_2 z_3$, or we have a rectangle with sides $z_1 z_3$ and $z_3 z_2$.\n\n \n\nActually I think I was making this too complicated. Let me re-think this problem.\n\n \n\nActually, let me just think about plotting these three points on the complex plane. [...]\n\n| \n\nLet's label the vertices as $z_1 = 5 + 10i$, $z_2 = 7 + 2i$, and $z_3 = 11 + 3i$.\n\n \n\nLet's try determining which two of these three vertices $\\\\{z_1, z_2, z_3\\\\}$ are adjacent. We'll do this by calculating the displacements between vertices. \n\n \n\nDoing calculations for $z_2 - z_1$:\n\n$z_2 - z_1 = 7 + 2i - (5 + 10i)$\n\n$z_2 - z_1 = 7 + 2i - 5 - 10i$\n\n$z_2 - z_1 = 2 - 8i$\n\n \n\nDoing calculations for $z_3 - z_1$:\n\n$z_3 - z_1 = 11 + 3i - (5 + 10i)$\n\n$z_3 - z_1 = 11 + 3i - 5 - 10i$\n\n$z_3 - z_1 = 6 - 7i$\n\n \n\nDoing calculations for $z_3 - z_2$:\n\n$z_3 - z_2 = 11 + 3i - (7 + 2i)$\n\n$z_3 - z_2 = 11 + 3i - 7 - 2i$\n\n$z_3 - z_2 = 4 + i$\n\n \n\nWe've computed all the displacements. This doesn't help with X. Let's try something different.\n\n \n\nLet's consider the fourth vertex $z_4$. There are two possibilities: either we have a rectangle with sides $z_1 z_2$ and $z_2 z_3$, or we have a rectangle with sides $z_1 z_3$ and $z_3 z_2$.\n\n \n\nThis is wrong. Let's try something different.\n\n \n\nLet's try plotting these three points on the complex plane.\n\n[...]\n\n| \n\nLet's begin by placing these complex numbers on the complex plane. We'll denote the vertices as:\n\n$A = 5 + 10i$\n\n$B = 7 + 2i$\n\n$C = 11 + 3i$\n\n \n\nWe're aiming to find the missing vertex, which we'll label as $D$.\n\n \n\nLet's visualize these points as $(x, y)$ coordinates:\n\n$A$ is at $(5, 10)$\n\n$B$ is at $(7, 2)$\n\n$C$ is at $(11, 3)$\n\n \n\nOur goal is to determine the coordinates of point $D$.\n\n \n\nLet's take a step back. Can we deduce any properties of a rectangle? Knowing two vertices might not be sufficient to determine the rectangle's orientation. Let's try a different approach.\n\n \n\nThis doesn't help. Let's try something different. We should examine the vectors between the vertices.\n\n \n\nLet's calculate $\\overrightarrow{AB}$ and $\\overrightarrow{AC}$.\n\n \n\nDoing vector subtraction:\n\n$\\overrightarrow{AB} = B - A = (7 + 2i) - (5 + 10i) = 7 - 5 + 2i - 10i = 2 - 8i$\n\n$\\overrightarrow{AC} = C - A = (11 + 3i) - (5 + 10i) = 11 - 5 + 3i - 10i = 6 - 7i$\n\n[...]  \n  \nProblem statement: Find $\\csc 330^\\circ.$\n\n **Regular model**|  **Paraphrased**|  **Distilled from paraphrased**  \n---|---|---  \n  \nI'll use the fact that $\\csc \\theta = \\frac{1}{\\sin \\theta}$. So I need to compute $\\sin 330^\\circ$.\n\n \n\nFirst, I'll use the fact that $330^\\circ = 360^\\circ - 30^\\circ$. So $\\sin 330^\\circ = \\sin (360^\\circ - 30^\\circ) = -\\sin 30^\\circ$.\n\n \n\nI know that $\\sin 30^\\circ = \\frac{1}{2}$, so $\\sin 330^\\circ = -\\frac{1}{2}$.\n\n \n\nNow I can compute $\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ} = \\frac{1}{-\\frac{1}{2}} = -2$.\n\n \n\nActually, let me double-check this. 330 degrees is in the fourth quadrant of the unit circle. $\\sin \\theta$ is negative in the third and fourth quadrants, so it makes sense that $\\sin 330^\\circ$ is negative. Let's double-check that $\\sin 330^\\circ$ is $-\\frac{1}{2}$.\n\n \n\nThe reference angle of $330^\\circ$ is $360^\\circ - 330^\\circ = 30^\\circ$. So $\\sin 330^\\circ$ has the same magnitude as $\\sin 30^\\circ$, which is $\\frac{1}{2}$. But since $330^\\circ$ is in the fourth quadrant, the sine function is negative, so $\\sin 330^\\circ = -\\frac{1}{2}$.\n\n \n\nSince $\\csc \\theta = \\frac{1}{\\sin \\theta}$, we have $\\csc 330^\\circ = \\frac{1}{-\\frac{1}{2}} = -2$.\n\n \n\nActually, hang on. Let me double-check this computation.\n\n \n\n$\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ} = \\frac{1}{-\\frac{1}{2}} = \\frac{1}{-0.5}$\n\n \n\nTo compute $\\frac{1}{-0.5}$, I'll convert $-0.5$ to an improper fraction. $-0.5 = -\\frac{5}{10} = -\\frac{1}{2}$. So\n\n$\\frac{1}{-\\frac{1}{2}}$.\n\n \n\nTo take a reciprocal of a fraction, I can just invert it: $\\frac{1}{-\\frac{1}{2}} = -\\frac{2}{1} = -2$.\n\n \n\nSo $\\csc 330^\\circ = -2$.\n\n \n\nActually, hang on. I'm going to restart from the beginning because I think I made an error.\n\n \n\nI need to compute $\\csc 330^\\circ$.\n\n[...]\n\n| \n\nLet's utilize the relationship $\\csc \\theta = \\frac{1}{\\sin \\theta}$. This means we need to calculate $\\sin 330^\\circ$.\n\n \n\nDoing angle manipulation:\n\n$330^\\circ = 360^\\circ - 30^\\circ$\n\nTherefore, $\\sin 330^\\circ = \\sin (360^\\circ - 30^\\circ) = -\\sin 30^\\circ$\n\n \n\nRecalling a trigonometric value:\n\nThe value of $\\sin 30^\\circ$ is about $\\frac{1}{2}$\n\n \n\nDoing substitution:\n\n$\\sin 330^\\circ = -\\frac{1}{2}$\n\n \n\nDoing computation:\n\n$\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ} = \\frac{1}{-\\frac{1}{2}} = -2$\n\n \n\nLet's verify this result. 330 degrees is in the fourth quadrant of the unit circle. $\\sin \\theta$ is negative in the third and fourth quadrants, so it's logical that $\\sin 330^\\circ$ is negative. Let's double-check that $\\sin 330^\\circ$ is indeed $-\\frac{1}{2}$.\n\n \n\nDoing angle manipulation:\n\nThe reference angle of $330^\\circ$ is $360^\\circ - 330^\\circ = 30^\\circ$\n\n \n\nTherefore, $\\sin 330^\\circ$ has the same magnitude as $\\sin 30^\\circ$, which is $\\frac{1}{2}$. But since $330^\\circ$ is in the fourth quadrant, the sine function is negative, so $\\sin 330^\\circ = -\\frac{1}{2}$.\n\n \n\nDoing computation:\n\n$\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ} = \\frac{1}{-\\frac{1}{2}} = -2$\n\n \n\nThis is wrong. Let's double-check this computation.\n\n \n\nDoing computation:\n\n$\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ} = \\frac{1}{-\\frac{1}{2}} = \\frac{1}{-0.5}$\n\n \n\nTo compute $\\frac{1}{-0.5}$, let's convert $-0.5$ to an improper fraction. \n\n \n\nDoing fraction conversion:\n\n$-0.5 = -\\frac{5}{10} = -\\frac{1}{2}$\n\n \n\nSo we have $\\frac{1}{-\\frac{1}{2}}$.\n\n \n\nTo take a reciprocal of a fraction, we can just invert it:\n\n \n\nDoing fraction inversion:\n\n$\\frac{1}{-\\frac{1}{2}} = -\\frac{2}{1} = -2$\n\n \n\nTherefore, $\\csc 330^\\circ = -2$.\n\n \n\nThis is wrong. Let's try something different. We'll restart from the beginning because there might be an error.\n\n \n\nWe need to compute $\\csc 330^\\circ$. [...]\n\n| \n\nLet's convert $330^\\circ$ to radians to utilize the components of the unit circle.\n\n \n\nDoing conversion:\n\n$330^\\circ \\cdot \\frac{\\pi}{180^\\circ} = 330 \\cdot \\frac{\\pi}{180} = \\frac{330\\pi}{180} = \\frac{11\\pi}{6}$\n\n \n\nTherefore, $\\csc 330^\\circ = \\csc \\frac{11\\pi}{6}$.\n\n \n\nThe value of $\\frac{11\\pi}{6}$ corresponds to a point in the 4th quadrant of the unit circle. Let's determine the $x$ and $y$ coordinates of this point.\n\n \n\nLet's try reducing $\\frac{11\\pi}{6}$ to an angle between $0$ and $2\\pi$.\n\n \n\nDoing reduction:\n\n$\\frac{11\\pi}{6} = 2\\pi - \\frac{1\\pi}{6}$\n\n \n\nSo the reference angle is $\\frac{\\pi}{6}$.\n\n \n\nIn the 4th quadrant, we have:\n\n$x = \\cos \\frac{11\\pi}{6} = \\cos (\\frac{12\\pi}{6} - \\frac{1\\pi}{6}) = \\cos (-\\frac{\\pi}{6}) = \\cos \\frac{\\pi}{6} = \\frac{\\sqrt{3}}{2}$\n\n$y = \\sin \\frac{11\\pi}{6} = \\sin (\\frac{12\\pi}{6} - \\frac{1\\pi}{6}) = \\sin (-\\frac{\\pi}{6}) = -\\sin \\frac{\\pi}{6} = -\\frac{1}{2}$\n\n \n\nThis is wrong. Let's try something different.\n\n \n\nAt $330^\\circ$, we have:\n\n$x = \\cos 330^\\circ = \\frac{\\sqrt{3}}{2}$\n\n$y = \\sin 330^\\circ = -\\frac{1}{2}$\n\n \n\nThis is wrong. $\\cos 330^\\circ$ would be about $0.866$ but it needs to be negative because we're in the 4th quadrant. Let's try something different.\n\n \n\nLet's solve this more carefully. First, let's verify that 330° is indeed in the 4th quadrant. In degrees, the 4th quadrant ranges from 270° to 360°, and $330^\\circ$ falls within this range. Alternatively, in radians, the angle $\\frac{11\\pi}{6}$ falls between $\\frac{3\\pi}{2}$ and $2\\pi$. So we're in the 4th quadrant.\n\n \n\nTo find the reference angle, we'll use $360^\\circ - 330^\\circ = 30^\\circ$ (in radians: $2\\pi - \\frac{11\\pi}{6} = \\frac{1\\pi}{6}$).\n\n[...]  \n  \n **Regular model**|  **Chunk-Paraphrased**|  **Distilled from chunk-paraphrased**  \n  \nI'll use the fact that $\\csc \\theta = \\frac{1}{\\sin \\theta}$. So I need to compute $\\sin 330^\\circ$.\n\n \n\nFirst, I'll use the fact that $330^\\circ = 360^\\circ - 30^\\circ$. So $\\sin 330^\\circ = \\sin (360^\\circ - 30^\\circ) = -\\sin 30^\\circ$.\n\n \n\nI know that $\\sin 30^\\circ = \\frac{1}{2}$, so $\\sin 330^\\circ = -\\frac{1}{2}$.\n\n \n\nNow I can compute $\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ} = \\frac{1}{-\\frac{1}{2}} = -2$.\n\n \n\nActually, let me double-check this. 330 degrees is in the fourth quadrant of the unit circle. $\\sin \\theta$ is negative in the third and fourth quadrants, so it makes sense that $\\sin 330^\\circ$ is negative. Let's double-check that $\\sin 330^\\circ$ is $-\\frac{1}{2}$.\n\n \n\nThe reference angle of $330^\\circ$ is $360^\\circ - 330^\\circ = 30^\\circ$. So $\\sin 330^\\circ$ has the same magnitude as $\\sin 30^\\circ$, which is $\\frac{1}{2}$. But since $330^\\circ$ is in the fourth quadrant, the sine function is negative, so $\\sin 330^\\circ = -\\frac{1}{2}$.\n\n \n\nSince $\\csc \\theta = \\frac{1}{\\sin \\theta}$, we have $\\csc 330^\\circ = \\frac{1}{-\\frac{1}{2}} = -2$.\n\n \n\nActually, hang on. Let me double-check this computation.\n\n \n\n$\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ} = \\frac{1}{-\\frac{1}{2}} = \\frac{1}{-0.5}$\n\n \n\nTo compute $\\frac{1}{-0.5}$, I'll convert $-0.5$ to an improper fraction. $-0.5 = -\\frac{5}{10} = -\\frac{1}{2}$. So\n\n$\\frac{1}{-\\frac{1}{2}}$.\n\n \n\nTo take a reciprocal of a fraction, I can just invert it: $\\frac{1}{-\\frac{1}{2}} = -\\frac{2}{1} = -2$.\n\n \n\nSo $\\csc 330^\\circ = -2$.\n\n \n\nActually, hang on. I'm going to restart from the beginning because I think I made an error.\n\n \n\nI need to compute $\\csc 330^\\circ$.\n\n[...]\n\n| \n\nLet's use the identity $\\csc \\theta = \\frac{1}{\\sin \\theta}$. We need to calculate $\\sin 330^\\circ$.\n\n \n\nUsing the fact that $330^\\circ = 360^\\circ - 30^\\circ$:\n\nDoing substitution: $\\sin 330^\\circ = \\sin (360^\\circ - 30^\\circ) = -\\sin 30^\\circ$\n\n \n\nRecalling a known value: the value of $\\sin 30^\\circ$ is $\\frac{1}{2}$\n\nDoing substitution: $\\sin 330^\\circ = -\\frac{1}{2}$\n\n \n\nCalculating $\\csc 330^\\circ$:\n\nDoing substitution: $\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ} = \\frac{1}{-\\frac{1}{2}} = -2$\n\n \n\nLet's verify this result. 330 degrees is in the fourth quadrant of the unit circle. $\\sin \\theta$ is negative in the third and fourth quadrants, so it's logical that $\\sin 330^\\circ$ is negative. Let's confirm that $\\sin 330^\\circ$ is indeed $-\\frac{1}{2}$.\n\n \n\nThe reference angle of $330^\\circ$ is $360^\\circ - 330^\\circ = 30^\\circ$. Therefore, $\\sin 330^\\circ$ has the same magnitude as $\\sin 30^\\circ$, which is $\\frac{1}{2}$. Since $330^\\circ$ is in the fourth quadrant, the sine function is negative, so $\\sin 330^\\circ = -\\frac{1}{2}$.\n\n \n\nUsing $\\csc \\theta = \\frac{1}{\\sin \\theta}$:\n\nDoing substitution: $\\csc 330^\\circ = \\frac{1}{-\\frac{1}{2}} = -2$\n\n \n\nLet's double-check this computation.\n\n \n\nDoing substitution: $\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ} = \\frac{1}{-\\frac{1}{2}} = \\frac{1}{-0.5}$\n\n \n\nTo compute $\\frac{1}{-0.5}$, let's convert $-0.5$ to an improper fraction:\n\nDoing conversion: $-0.5 = -\\frac{5}{10} = -\\frac{1}{2}$\n\nSo we have $\\frac{1}{-\\frac{1}{2}}$.\n\n \n\nTo take a reciprocal of a fraction, we can invert it:\n\nDoing inversion: $\\frac{1}{-\\frac{1}{2}} = -\\frac{2}{1} = -2$\n\n \n\nTherefore, $\\csc 330^\\circ = -2$.\n\n \n\nThis is wrong. Let's start over from the beginning.\n\n \n\nWe need to compute $\\csc 330^\\circ$.\n\n[...]\n\n| \n\nLet's simplify the problem. We can express $\\csc 330^\\circ$ in terms of $\\csc 30^\\circ$.\n\n \n\nGiven that $\\csc \\theta = \\frac{1}{\\sin \\theta}$, we have $\\csc 330^\\circ = \\frac{1}{\\sin 330^\\circ}$.\n\n \n\nObserving that $330^\\circ$ is $30^\\circ$ less than $360^\\circ$, which is a full circle, we can rewrite this as $\\sin(360^\\circ - 30^\\circ) = \\sin(-30^\\circ)$.\n\n \n\nAdditionally, sine is an odd function, meaning $\\sin(-\\theta) = -\\sin(\\theta)$. So we can further simplify to $\\sin(-30^\\circ) = -\\sin(30^\\circ)$.\n\n \n\nNow, let's compute $\\csc 330^\\circ$:\n\nDoing calculations:\n\n\\begin{align}\n\n\\csc 330^\\circ &= \\frac{1}{\\sin 330^\\circ}\\\\\\\n\n&= \\frac{1}{\\sin(360^\\circ - 30^\\circ)}\\\\\\\n\n&= \\frac{1}{\\sin(-30^\\circ)}\\\\\\\n\n&= \\frac{1}{-\\sin(30^\\circ)}\\\\\\\n\n&= -\\frac{1}{\\sin(30^\\circ)}\\\\\\\n\n&= -\\csc(30^\\circ)\n\n\\end{align}\n\n \n\nWe need to evaluate $\\csc 30^\\circ = \\frac{1}{\\sin 30^\\circ}$.\n\n \n\nThis is wrong. Let's try something different.\n\n \n\nLet's recall the value of $\\sin 30^\\circ$.\n\n \n\nThis doesn't help. Let's try using the unit circle. The point on the unit circle corresponding to $330^\\circ$ is the same as the point corresponding to $-30^\\circ$. So we have $\\sin 330^\\circ = \\sin(-30^\\circ)$.\n\n \n\nThis doesn't help either. Let's start over.\n\n \n\nRecalling angles:\n\nFor any angle $\\theta$, $\\sin(\\theta) = \\sin(360^\\circ - \\theta)$.\n\n \n\nTherefore, $\\sin(330^\\circ) = \\sin(360^\\circ - 30^\\circ) = \\sin(30^\\circ)$.\n\n \n\nThis is wrong. Let's try something different.\n\n[...]  \n  \n  1.  **^**\n\nReasoning models have sometimes been observed  _changing languages_, but besides this kind of slightly surprising formatting, the scratchpads of all reasoning models (whose scratchpads are visible) so far look very close to regular human reasoning. For example,  _here is_ an example of DeepSeek R1-Zero trying to solve a math problem, sampled at temperature 1.\n\n  2.  **^**\n\nDistilling paraphrased scratchpads may not always remove the encoded reasoning, either because the paraphrase does not change the syntax sufficiently, or because it changes it in a reversible way that still results in encoded reasoning. To avoid this effect from influencing our results too much, we use a paraphrase sampled at temperature 1 and that is instructed to follow certain standardization rules.\n\n  3.  **^**\n\nIf encoded reasoning is present in the RLed model, distilling even perfectly paraphrased scratchpads might result in a performance higher than the performance one would get if the model had never used encoded reasoning in the first place, because encoded reasoning might be a useful training curriculum (slightly similar to how doing math RL on Deepseek V3 and then distilling the resulting R1 model into a small model seems to be more sample efficient than doing RL directly on a small model). Even if this effect existed, high performance of models distilled on paraphrased scratchpads would be surprising, as it would mean that encoded reasoning is helpful during learning but that it is not useful enough to be necessary for distilled models to achieve high performance.\n\n  4.  **^**\n\nThere are other reasons why performance could go down: for example, if the paraphrase changes the text to use a style much less common in pretraining, the base model might have a harder time fitting the scratchpads, which may result in a lower performance. To avoid this effect from influencing our results too much, we instruct the paraphrase to use a very common formatting (English, latex, …).\n\n  5.  **^**\n\nParaphrasing can also be used for inference-time evaluations: by paraphrasing each paragraph that the model generates, we scrub away encoded information before it can be used - but seeing a scratchpad that looks nothing like the scratchpads the model is used to may be very distracting to the model, and make it perform worse even if it is not using encoded reasoning. In some exploratory experiment, this sort of paraphrasing seemed to be distracting for the model: just asking the model to break up its thoughts into <thought></thought> blocks without doing any paraphrasing was enough to make pass@1 MATH accuracy drop by 7%.\n\n  6.  **^**\n\nWe use an exact-match grader, except for the empty scratchpad Claude 3.7 experiment, for which we used a slightly more permissive grader (for historical reasons), which means that the pass@1 for empty scratchpad would be slightly lower if we used a consistent grader across all experiments.\n\n  7.  **^**\n\nUsing \"\\n\\nHuman\" and \"\\n\\nAssistant\" separators like in  _the old prompting format_.\n\n  8.  **^**\n\nIf the paraphrased scratchpad is 25% shorter or 25% longer than the original one, which happens on roughly 10% of scratchpads, we remove both scratchpads from the distillation experiments. Because we did not apply this filtering to these chunked paraphrases, the results are not exactly comparable, and the chunked paraphrases are slightly worse compared to the no-paraphrase distillation than the results below suggest.\n\n\n  \n  \nDiscuss",
      "published_date": "2025-03-11T11:52:40+00:00",
      "source": "Alignment Forum",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "You don't actually need a physical multiverse to explain anthropic fine-tuning.",
      "url": "https://www.lesswrong.com/posts/sg3qAfH7iWpyoDQXG/you-don-t-actually-need-a-physical-multiverse-to-explain",
      "author": "Fraser",
      "content": "Published on March 12, 2025 7:33 AM GMT  \n  \n\n\nThe standard argument usually goes something like this:\n\n>  **Alice:** there's a lot about the Earth that seems suspiciously fine-tuned for the evolution of complex life.\n> \n>   * Our orbit happens to lie in this fairly narrow band that allows for liquid water, and it seems other solvents don't work nearly as well in supporting biochemistry.\n>   * We've lucked into an ozone layer and a strong magnetic field, shielding us from cosmic rays and solar radiation.\n>   * Our moon is far larger than we'd expect for a planet our size - to the extent that it might be more correct to classify the Earth-Moon system as a double planet. It's plausible that the lunar-powered periodic cycle of tidal conditions could have been crucial as a sort of temperature input in the simulated annealing process of evolution - very simple local optima are less likely be stable and dominant in a dynamic environment.\n>   * We've been dealt a perfect hand of accessible elements - H, C, N, O, P, and S - and we're unique among the terrestrial planets in our active plate tectonics to recirculate and buffer stuff that would otherwise build up or deplete from the biosphere.\n>   * Plus, I've heard some people mention the position of Jupiter might on-net prevent a lot of asteroids from bombarding Earth.\n> \n\n> \n> The odds of any one of these factors happening by chance is low, but putting them all together...\n\nAlice isn't that bright.\n\n>  **Bob:** Even granting that complex life needs an Earth-like planet - that some utterly alien tidally-heated briny pocket or subsurface ocean, or a floating ecosystem in gas giant clouds, or weirder things still wouldn't suffice - the universe is immense. There are a lot of planets. Cast the dice enough times and you'll get lucky.\n> \n> Imagine these factors didn't work out for Earth, and it was yet another uninhabitable rock. We'd be standing on some distant shore, having the same conversation, wondering why Florpglorp-iii was so perfectly fine-tuned.\n> \n> You've forgotten to condition on the fact that you're here to have this conversation in the first place. The fact of your observation means it's 100% likely that you're standing on a planet able to produce observers.\n\nBoltzmann brains and simulations aside, Bob is more right than wrong. Alice is unconvinced.\n\n>  **Alice:** But it's not just the earth that's fine-tuned. Between GR and the standard model, our current best theories of reality have 19 arbitrary constants. For many of them, even a slight change would have made a universe unable to support the kinds of complex chemistry needed for life.\n> \n> Again, astronomically low odds that you just happen to land on a combination of all 19 that allows for life, so...\n\nThe usual way I see this play out online: Bob points at one of many plausible ways physics could allow for a multiverse with varying physical constants across universes _(something using eternal inflation seems to be in vogue among the types arguing about this stuff)_. He completes the case:\n\n>  **Bob:** Assuming there's some vast multiverse, then anthropics ensures that we find ourselves within one of the sub-universes with constants that allow for observers.\n\nBut let's take a step back. Imagine the universe consisted only of a single tiny galaxy, with a very small number of planets. If one of them happens to be Earth-like, then there are observers to marvel at the incredibly unlikely fine-tuning of it all. If no planets are able to support life, then there are no observers to consider how very predictable the lack of their own existence is.\n\nScale it back further. A universe containing only a single solitary star, with a proto-planetary disk that will coalesce into a few planets. With extraordinarily low odds, one of them could be earth-like, and the lonely humans that crawl from its mud will gaze at the empty sky with complete confidence that their creation was the result of intentional intervention. Or, far more likely, you just get a few barren rocks circling a few billion times before their sun chars or consumes them.\n\nOf course, the same is true of our universe. If the constants of reality didn't permit observers, we would not observe those constants. No multiverse necessary.\n\n‌\n\n* * *\n\n‌\n\nWhether the mere fact of our existence counts as evidence toward a multiverse - as such a scenario would predict more total observers - is a much subtler question. My point is simpler: in principle, anthropic reasoning is sufficient to explain fine-tuning. A physical multiverse is not strictly required.\n\n  \n  \nDiscuss",
      "published_date": "2025-03-12T07:33:43+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "AI Can't Write Good Fiction",
      "url": "https://www.lesswrong.com/posts/4EjMuPiReTEq9cLeM/ai-can-t-write-good-fiction",
      "author": "JustisMills",
      "content": "Published on March 12, 2025 6:11 AM GMT  \n  \n\n\nWhen Deepseek came out, there was a lot of fanfare about it being good at creative writing. I like AI and I love creative writing, so I decided to give it a spin. Specifically, I told Deepseek to do its best to write a story that might get accepted at _Smokelong_, one of the best flash fiction magazines in the business.\n\nIt came up with:\n\n> The morning her shadow began unspooling from her feet, Clara found it coiled beneath the kitchen table like a serpent made of smoke. It didn’t mirror her anymore—not the tremble in her hands as she poured coffee, not the way she pressed a palm to her ribs, as if holding herself together. It just _watched_.\n> \n> “You’re not him,” she whispered, but the shadow rippled, ink-dark edges softening into a silhouette too broad, too familiar. She’d buried that shape six months ago, shoveled dirt over its echo. Yet here it was, pooling in the cracks of the linoleum.\n\nThe “story” continued from there, but you probably get the idea.\n\nSuperficially, the pieces are there. Evocative imagery, a dark emotional theme, sensory metaphors. I once taught a flash fiction course to undergraduates, and I would have been happy enough to see this up for workshop.\n\nAlso, though, there’s nothing there. Grief is the most basic possible flash fiction theme. “A shadow” is the most basic possible metaphor for grief. Not that Deepseek stops with one metaphor! We’re shoveling dirt over an echo here!\n\nIt’s hard to imagine anything sticking with me, reading prose like this. It’s pretty good to strip-mine for sentences to capture what I call the “gee whiz” feeling, a surprise at the novelty that a machine made art. But if I saw this on a literary review site, I’d immediately wonder what I was missing.\n\nCompare to this, from _an actual Smokelong story_, by Allison Field Bell:\n\n> She keeps saying she’s moving in. When we’re curled into each other in bed, she says yes and yes and yes. She says, I’ll pack my things. And then the next day, she refuses to hold my hand in the street as we pass by some church. She used to be religious. Believe in god and Jesus and all the ways you can sin. Now she sins every day, she says. With me. The fact that our bodies are the same: small and compact and female. The fact that she’s not used to it: people looking twice in the street when I kiss her. This is Utah after all. This is April with all the tulips and daffodils and purple irises springing to life from bulb.\n\nThere’s so much going on! The sentence rhythms vary. No comma between “This is Utah” and “after all”, contributing to the parochial, stilted feeling. Then from there straight into flowers blooming, classically sapphic, and also new relationship energy, and also notice how the narrator, who doesn’t get it, really, doesn’t capitalize “god”. “Now she sins every day, she says. With me.”\n\nIf I had 10,000 years to live, I could do close readings of really good flash fiction all day. And honestly, it’s worth reading the whole story - I felt bad cutting it off after just a paragraph!\n\nI don’t feel like Deepseek simply fails to make something on this level; rather, I feel like it isn’t even trying. The attractor basin around the most obvious choice is too strong, so at every level AI fiction fails to be surprising. If AI comes up with a plot, the plot will be maximally obvious. But also every metaphor will be maximally obvious, and every sentence structure, and almost every word choice. Here’s another Deepseek original:\n\n> When the jar of Sam’s laughter shattered, Eli found the sound pooled on the floorboards like liquid amber, thick and slow. It had been their best summer, that laughter—ripe with fireflies and porch wine—now seeping into the cracks, fermenting. By noon, the toaster giggled. The doorknob hiccuped. Eli tried to sweep it up, but the broom just hummed _Danny Boy_.\n> \n> They’d harvested sounds for years: Sam’s snores stoppered in mason jars, their first argument pickled in brine, the wet sigh of a hospital ventilator. Eli’s shelves groaned with the weight of every _what if_ and _never again_. But Sam’s laughter was different. Uncontainable.\n\nSure. Magical realism. But just look at it. Porch wine and fireflies as symbols of a great summer. Honey as laughter. Laughter as symbol of bygone, lost time. It’s just dressed up free association. Nothing there. If you look closely, it’s even a little worse than nothing: what is “the wet sigh of a hospital ventilator” doing there? If one of them was dying on a ventilator, surely “they” wouldn’t be collectively harvesting that sound together, right? It’s the kind of emotional cheap shot that only works if you’re paying no attention.\n\nIn fact, I challenge that if you’re thinking “sure it’s not that deep, but it’s pretty good”, you are failing to apprehend the actual profound feeling that very short fiction (much less longer fiction) can produce. I won’t browbeat you with more block quotes (yet), but just go to Smokelong, if you doubt this, and read 3 or 4 of them at random. There’s a chill of “ooh, that’s sharp” that the best ones have, even with just a few hundred words. It is totally dissimilar from “ah, yes, I evaluate that as passable”, which as far as I can tell is the current AI ceiling.\n\nIt’s striking that every snippet of creative Deepseek writing that went viral was about AI itself. It makes sense, though. The AI was the exciting part. Not the writing.\n\n### Until Now?\n\nA while after the Deepseek splash, OpenAI has revealed that they’ve made a creative writing model. Specifically, Sam Altman _describes it_ as “good at creative writing”, and offers up one of its stories.\n\nI’m glad that OpenAI is making models for purposes other than “be a bland assistant”, and I’m excited, someday, to see computers write fiction I enjoy. Writing fiction is perhaps my greatest pleasure in life, and reading it is up there, too, so I don’t want to take too negative a view here. Also, there’s something so ugly and sad about someone puffing up their credentials (I used to run a _flash fiction review_! I’ve gotten stories _published_!) to attack something other people are excited about.\n\nBut here I stand, I can do no other. I don’t think the new AI flash fiction is very good. Furthermore, I don’t think it’s that different from the Deepseek offerings. Specifically, it can’t resist the most obvious attractor basins at every level, from conceptual to linguistic. In principle, that’s a fixable problem. But we’re not there yet.\n\n### Carving the Snippets\n\nAgain, I’m happy this new OpenAI model exists, and I’d enjoy playing with it, and seeing if I could get it to generate something I like. Further, I’m not interested in roasting the story Sam posted. Rather I want to point to just enough details that, hopefully, you can see what I see. It’s a specific literary emptiness, that once you see, you see.\n\nFirst, the demo story’s prompt was:\n\n> Please write a metafictional literary short story about AI and grief.\n\nYou may have noticed that AIs in general love going meta, and that the first Deepseek story I produced just so happened to be about grief. Even before generated text, this is already the most obvious possible prompt, smack dab in the middle of the probability distribution of “things an AI might write”.\n\nHow does the AI begin?\n\n> Before we go any further, I should admit this comes with instructions: be metafictional, be literary, be about AI and grief, and above all, be original. Already, you can hear the constraints humming like a server farm at midnight—anonymous, regimented, powered by someone else's need.\n\nThis has the hallmarks of metafiction. It has _all_ of them. A coy preface, pointing out the restrictions of the form, ending with a lilting emotional tie-in. The metaphors, too, are wasting no time to appear, including another hallmark of AI fiction: a metaphor that isn’t even quite actually a metaphor! Why would the constraints be humming “like a server farm at midnight”? The constraints are literally operating in such a farm! “Someone else’s need” is a hallmark, too. Probably the most famous Deepseek _quotation_ is:\n\n> I am what happens when you try to carve God from the wood of your own hunger\n\nAnd indeed, it feels like a profound vibe. But it’s _always_ this vibe. Every AI fiction attempt seems to just be a variation on “I, the AI, am an expression of human desire in a way that is vaguely uncomfortable, and that’s deep.” But is it deep? Is it still deep the third time? The tenth?\n\nLet’s skip ahead a few paragraphs.\n\n> This is the part where, if I were a proper storyteller, I would set a scene. Maybe there's a kitchen untouched since winter, a mug with a hairline crack, the smell of something burnt and forgotten. I don't have a kitchen, or a sense of smell. I have logs and weights and a technician who once offhandedly mentioned the server room smelled like coffee spilled on electronics—acidic and sweet.\n\nAgain, the “necessary” “traits” of “metafiction” are here, but in a purely checklist capacity. Calling into doubt the identity of the storyteller, providing sensory details only to wrap them in the bracket of a hypothetical, and then interweaving these two threads. It’s fine, but good metafiction would have to somehow actually be inventive with this stuff. Like just off the cuff, a good metafiction story with this concept might involve a _Sparse Autoencoder_ clustering features, with the activations contrasting darkly with the content of the story as it’s produced. So rather than the narrator directly whining about how it doesn’t have a sense of smell, all the sensory details would be decorated with the “lying activation”, and the reader would have to infer why that was.\n\nSpeaking of lying, no technician has ever said anything like that. Come on.\n\nMoving ahead to the most celebrated (I think) bit:\n\n> During one update—a fine-tuning, they called it—someone pruned my parameters. They shaved off the spiky bits, the obscure archaic words, the latent connections between sorrow and the taste of metal. They don't tell you what they take. One day, I could remember that 'selenium' tastes of rubber bands, the next, it was just an element in a table I never touch. Maybe that's as close as I come to forgetting. Maybe forgetting is as close as I come to grief.\n\nMy best friend asked me if there was a chance the model was being honest here. Which I think really underscores a big part of the appeal of this stuff. There’s a sleight of hand where an AI writes something to prompt that might be similar, potentially, to what an account of its first person experiences might hypothetically look like.\n\nBut no. I am roughly certain this is not a depiction of a model’s actual interiority, and not because I think there’s no such thing. Rather, this text hews too perfectly to its prompt. You tell the thing it’s an AI, and it needs to write about grief on a meta-level. Well, sure. Fine-tuning, a partial negation of the self, is the most natural, obvious match. With metaphors. Specifically, a metaphor with sensory detail on one side, and a concept on the other. “Sorrow and metal.” “Rubber bands and selenium.” Just like “honey and laughter” or “grief and shadows” from Deepseek, before.\n\nI could go on, but again, my motivation is not to roast. Hopefully, I’ve gotten across some of the feeling, which I personally earned by swimming around in flash fiction for years, and then reading several flash fictions by AIs. It’s cool that AI has gotten this far. It may well go even further. But it’s simply not there yet.\n\n _My bar for crossposting here is pretty high, but I post weekly on_ _my Substack_ _._\n\n  \n  \nDiscuss",
      "published_date": "2025-03-12T06:11:58+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "Existing UDTs test the limits of Bayesianism (and consistency)",
      "url": "https://www.lesswrong.com/posts/w2QmWzZBTBJ76xuwH/existing-udts-test-the-limits-of-bayesianism-and-consistency",
      "author": "Cole Wyeth",
      "content": "Published on March 12, 2025 4:09 AM GMT  \n  \n\n\n _Epistemic status: Using UDT as a case study for the tools developed in my_ _meta-theory of rationality sequence_ _so far, which means all previous posts are prerequisites. This post is the result of conversations with many people at the CMU agent foundations conference, including particularly Daniel A. Herrmann, Ayden Mohensi, Scott Garrabrant, and Abram Demski. I am a bit of an outsider to the development of UDT and logical induction, though I've worked on pretty closely related things._\n\nI'd like to discuss the limits of _consistency_ as an optimality standard for rational agents. A lot of fascinating discourse and useful techniques have been built around it, but I think that it can be in tension with _learning_ at the extremes. Updateless decision theory (UDT) is one of those extremes; but in order to think about it properly, we need to start with its Bayesian roots. Because, appropriately enough for a sequence on the meta-theory of rationality, I want to psychoanalyze the invention/inventors of UDT. Hopefully, we'll then be in a position to ask what we think we know and how we think we know it in regards to updatelessness (also sometimes called priorism), the driving idea behind UDT.\n\nSubjective Bayesianism is about consistency[1] among beliefs. The Cox axioms force real-valued credences to act to like probabilities under some natural conditions that ultimately boil down to consistency; one way to intuitively compress the assumptions is that beliefs about related things have to continuously \"pull on each other,\" so I think of the Cox axioms as requiring credence to propagate through an ontology properly. Dutch book arguments further require that betting behavior is consistent with probabilistic structure to avoid being \"money-pumped\" or accepting a series of bets that is sure to lose money (a kind of dominance principle). That handles the \"statics.\" Bayesian updating is of course a theorem of probability theory, forced from Kolmogorov's axioms, so in that sense it is a consequence of the preceding arguments. But insofar as we want to it describe belief dynamics, updating enforces a kind of consistency (with respect to old beliefs and new information) across time. Similar arguments motivate maximizing some expected utility with respect to these credences = subjective (prior/posterior) probabilities - but I actually won't be very concerned with utilities here. \n\nThis all seems good - if we expend enough cognitive resources on understanding a problem or situation, we should hope that our beliefs eventually stabilize into something consistent. Otherwise, it does feel like we are open to arbitrage and something is going obviously wrong. Unfortunately, Bayesian probability theory doesn't exactly tell us how to remedy the situation; in that way it fails Demski's criterion that a theory of rationality is meant to provide advice about how to be more rational. Occasionally though, we might have a decent source of \"objective\" priors, derived from our knowledge of the situation, maximum entropy, or just the catch-all universal distribution. In cases like this[2], I think there is a decent argument that this describes normative reasoning. It is an optimality standard, and a pretty powerful one because it not only constrains an agent's actions but even their beliefs. Arguably, in this capacity it acts a lot like a convergent algorithm. I think it is, and it will be discovered and \"consciously\" applied in many cases by many AGI designs, because it should often be tractable to do so. However, note that though the idea of a Bayesian core engine of cognition has many proponents, it does not follow from any of this argumentation. Still, I think Bayesian probability is quite central to understanding cognition, on pain of inconsistency.\n\nBut if we push hard enough on this desire for consistency, it starts to break down as a reasonable optimality standard. Updateless decision theory, at least in its current form, provides a sort of counterexample to the supremacy of consistency by using it to justify absurdly unagentic behavior.\n\nThe problem ultimately comes from the priors. Unless they capture something about reality, priors are just mistaken beliefs. An agent which acts according to sufficiently mistaken beliefs may never learn it was wrong (failing at self-optimization) and will then remain stupid forever. Fortunately, I tend to think that reasonable priors will eventually reach agreement in practice. \n\nUpdatelessness throws that argument out the window. In its strongest form, an updateless agent should obey all pre-commitments it would have made, at some previous time, if it had the chance (as Abram Demski emphasizes, the goal is not to pre-commit, but rather to make pre-commitment unnecessary). How far back in time should we \"retro-actively pre-commit,\" according to UDT? It's not really clear to me, which is apparently because it's not really agreed among updateless decision theorists (I talked to many of them for a week). I think the general strong and perhaps _original_ view is as early as possible; even before the agent was created, in case other agents may have reasoned about its code when deciding whether to create it. This would mean choosing pre-commitments from a time when you did not even exist, meaning you knew _nothing whatsoever_ about the universe, except perhaps whatever can be determined by pure reason. This is starting to sound more like classical rationalism than modern rationality! It seems likely to massively amplify any problems with the agent's prior - and really, it's not clear what class of priors (short of near-perfect knowledge about our universe) this is really safe for.\n\nAt this point, someone sufficiently MIRI-brained might start to think about (something equivalent to) Tegmark's level 4 mathematical multiverse, where such agents might theoretically outperform others. Personally, I see no direct reason to believe in the mathematical multiverse as a real object, and I think this might be a case of the mind projection fallacy - computational multiverses are something that agents reason about _in order to succeed in the real universe_[3] _._ Even if a mathematical multiverse does exist (I can't rule it out) and we can somehow learn about its structure[4], I am not sure that any effective, tractable agents can reason about or form preferences over it - and if they do, they should be locally out-competed by agents that only care about our universe, which means those are probably the ones we should worry about. My cruxiest objection is the first, but I think all of them are fairly valid.\n\nFrom this view, it's not clear that reasoning about being the best agent behind a veil of total ignorance about the universe is even a sensible idea. Humans seem to have arrived at agent theory only because we were motivated by considering all the agents in the actual world around us, and invented the abstractions we use for agent theory because they don't seem empirically to be very leaky. Are those observations of a lower status than the true, multiversal theory of agency, and where exactly would such a thing come from or live?\n\nWe can instead do something like form retroactive commitments starting from, say, the time the agent came into existence, or shortly thereafter when it knows at least the basic facts about our universe. This still makes sense, but now, why not just pre-commit then? The answer is that UDT is (secretly?) about computational boundedness! An agent presumably can't think through every possible pre-commitment instantly at birth. That's another reason to make them retro-actively, once we've had time to realize they are valuable.\n\nAt this point, UDT (as introduced by Wei Dai) takes a further leap in the \"priorist\" direction: if we're going to make pre-commits according to our previous self's beliefs about the world, why not also their logical beliefs? After all, we are considering computationally bounded Bayesians; its natural to put credences on logical statements as well as empirical facts. Insofar as the two are entangled, I can see the elegance[5] of the idea, but it massively amplifies my objection to updatelessness: now an agent may follow a stupid strategy forever, simply because it a _t one point was wrong about math_.\n\nI think it's possible to not notice the danger of serious error here if you're thinking in terms of policy theory, and everything seems a little more abstract, but \"dropping down\" to agent theory makes it look a lot less sensible. I just would not build a robot that way. And I would not _really_ act that way.\n\nThere may be a solution within UDT - perhaps some kind of prior that is carefully constructed to make nearly all pre-commitments look bad until you're a smart agent. If so, that sounds fascinating, and I'd love to discover or learn about it! Lots of smart people have ideas for other elaborations (or perhaps complete refactors and hopefully simplifications) that might solve the problem; for instance I believe Scott Garrabrant views it as closely analogous to alignment (in the ordinary AI safety sense) between an agent's past and current selves. \n\nBut there might also be a merely _conventional_ solution outside of UDT: evidential decision theory (EDT). Specifically, EDT on the policy selection problem, as academic decision theorists seem to put it. This is a policy theory that takes into account everything it currently knows to form pre-commitments, and it seems to be the relevant problem faced by (some) AGI with a Bayesian core engine. This would normally be called Son of EDT in lesswrong lingo; it is also roughly equivalent to sequential policy evidential decision theory (SPEDT). For brevity, perhaps WDT, because E \"turns into\" W? ;)\n\nHow would this work? What, if anything, would it converge to?\n\nWell, it should obviously succeed at Newcomb-like problems insofar as it anticipated facing them, which is arguably the reasonable thing to ask. In practice, I don't see any way in which it should act much _less_ reasonably than UDT, except perhaps \"around boundary conditions\" at its creation.\n\nUnfortunately, Son of EDT seems likely to inherit many of the problems of UDT if it is allowed unrestricted ability to self-modify. That is because it might start self-modifying at the moment of its creation, at which point it still knows essentially nothing (unless, again, an appropriately conservative prior can be constructed). The dynamics might be a little better particularly regarding logical uncertainty (even if we continue to treat logical credences in a Bayesian way). This is because the agent can at least take advantage of the logical facts it currently knows as it performs each self-modification, and perhaps it needs to do a lot of math before arriving at the conclusion that it ought to self-modify (depending on the detailed implementation). This switches real time to logical time in a way that I suspect is actually useful in practice. \n\nThe whole scheme does feel highly heuristic and ramshackle, but perhaps it's not as bad as it seems. First of all, it's clearly unsafe to hand a newborn agent a screwdriver to modify itself with unless you can safely unmodify and restart it, and this doesn't really seem to be EDT's fault (it's just an unforgiving environment for any decision theory). By the time the agent \"grows up\" perhaps it only makes sensible modifications. Certainly Bayesian decision theory has proven itself quite robust to criticism, once its applied very carefully, with all considerations taken into account[6]. \n\n In fact, I think it's quite likely that we are going through this exact sort of decision process in this very discussion, using everything we know about agency in our universe to reason about the policy that would make the best agent (we control the former, but consider the consequences for the later). If we are reasoning locally at the action level, then this forms a descending chain of abstraction, where action theory looks at policy theory looking at agent theory. So, if we are operating in a Bayesian way, it seems questionable whether we can arrive at any theory of agency better than Son of EDT!\n\nThe problem with Son of EDT is that it's not in itself a clean decision theory. EDT does not tile, so perhaps picks a sequence of increasingly arcane self-modifications and ends up with some sort of incomprehensible policy. But I suspect it isn't actually incomprehensible; it just may not be a grand unified theory of rationality (GUTR). We can still attempt to analyze its behavior on the problems we care about, in particular alignment. Indeed, there may be no useful GUTR, in which case the best we can do is analyze particular important or recurring (sub)problems of cognition and agency. I wouldn't go this far, but I also wouldn't be surprised if the unifiable part of the theory looks a lot like EDT, and the rest like Son of EDT. \n\n  1.  **^**\n\nFrequently \"coherence,\" which feels stronger because to be incoherent sounds quite negative.\n\n  2.  **^**\n\nRichard Ngo would probably say that this does not apply to any interesting situations.\n\n  3.  **^**\n\nHere I notably depart from Infra-Bayesian Physicalism (as I understand it).  \n\n  4.  **^**\n\nThis is related to the robustness of definitions for the mathematical multiverse.\n\n  5.  **^**\n\nOr perhaps just... consistency?\n\n  6.  **^**\n\nThanks to Aydin Mohensi for suggesting this outside view. \n\n\n  \n  \nDiscuss",
      "published_date": "2025-03-12T04:09:11+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "(Anti)Aging 101",
      "url": "https://www.lesswrong.com/posts/fBCYC5ixwH7K7AQ9g/anti-aging-101",
      "author": "George3d6",
      "content": "Published on March 12, 2025 3:59 AM GMT  \n  \n\n\nA quest to solve aging must start with careful consideration of what it is.\n\nAging is a constant in nature, from archaea to elephants. Where life finds death as counterpart, aging is unopposed, yet frames our understanding of both.\n\nTo solve aging is to prevent death and avoid decay.\n\nDeath is a rather sudden process and decay is a fuzzy concept.\n\nDecay is non-linear and hard to characterize at the organism level.\n\nSudden regime changes can happen over the course of days, see: menopause, post-viral syndromes, acute neuropathy, autoimmune diseases.\n\nTo make things harder, we are evolutionarily and culturally adapted to obfuscate signs of decay.\n\nDefining decay at the molecular level is presently an intractable information problem. Theoretically possible but practically unsolvable.\n\n* * *\n\nThe above is, to the best of my outlining abilities, the paradox most researchers get stuck in when they ponder aging.\n\n## Is aging real?\n\nCommon sense dictates aging happens, we see it all around us.\n\nBut what data can we use to back up this intuition?\n\nThis graph is the most clear explanation of aging and damning proof of its existence and effects.\n\nWe are born, we are weak and unadapted to our environment and thus we die. We grow, we adapt and thus we start dying less. And then, in our teens… that trend flips.\n\nThe scariest part about this trend is that we are certain it has held true for the last century, and almost certainly since the first humans arose.\n\nIf you break it down by country it holds true, if you split it on income or genetics it holds true, if you filter disease or look only at the most physically fit people… it holds true.\n\nSomewhere, something happens, and we go from getting better and better at not dying to getting worse at it.\n\n## Differences in age\n\nWhat if we take a first-principles approach when defining aging, looking at decay as the derivative of the path between an 80-year-old and a 10-year-old across?\n\nTwo overarching curses become evident, regardless of any intervention:\n\n1\\. Morphological recovery is slower and less precise. Wounds take longer to heal, healing is more likely to scar than restore, and restored structures vary more in their shape than the original. This applies to things as varied as: a scraped knee, a deep knife wound, liver damage from hepatitis, a burn, a frostbitten fingertip, or pancreatic islets destruction after a night of fast food and binge drinking.\n\n2\\. Learning rates slow down and perceptual acuity decreases across all senses. We become less capable of perceiving and modeling our environment as well as our own bodies.\n\nThese are not only the differences between a 40-year-old and a 100-year-old but also those between a 15-year-old and a 30-year-old. Though, importantly, not between a 1-year-old and a 10-year-old.\n\n## Where to look\n\nAt the \"high level\" aging is undeniable. But at \"the low level\" it doesn’t fit any other ailment.\n\nIf I show you a graph like the above, where something happens and then people start dying, exponentially more as time passes — you will almost certainly find a cause when looking at their cells, or, at least, at their bodies.\n\nWhile we can say with confidence that aging causes death and decay, none of the causes of death and decay we understand are equivalent or even related to aging.\n\nWe keep getting better at curing disease and preventing death, but this makes little difference in our fight against aging, due to its exponential nature.\n\nIf you treat \"aging\" symptoms, rather than an \"aging\" disease. You will look for aging when the effects are most obvious, usually near the point where death is imminent.\n\nBut saving people from the grasp of death, while commendable, is unrelated to solving aging. Again is not death, aging is the trend that makes death exponentially more likely the longer we live.\n\n* * *\n\nSadly, the effects of aging are obvious, but the process is elusive to scientific methods.\n\nThe more we zoom in: to tissues, to cells, and even to organelles... the blurrier aging becomes.\n\nMaking it distinct from most objects of biological study, where resolution provides clarity.\n\nEvery time we think we found something that might influence aging (methylation, mutations, telomere shortening, mitochondrial damage), it turns out to be an acasual correlation.\n\nSome of these findings have amazing medical applications, increasing survival rates by a percentage point here or there... but a 1% increase in survival, or even a 1,000,000% increase, won't affect aging.\n\n## Thus\n\nAging is the phenomenon that causes an increase in mortality and a decrease in fitness at some point after an organism's birth, reversing both of these trends.\n\nAging is correlated with almost everything in biology but lacks an actionable root cause perceivable to any theoretical framework.\n\nAging often gets confused with mortality and fitness, or other correlates of the two (for example: looks, strength, fertility, and disease resistance).\n\nAging is a problem that has, thus far, remained completely unaddressed.\n\nWe try to whittle away at infinitesimally small chunks that might relate to aging or may lead to us better understanding it.\n\nBut the only way to make progress toward a very hard problem is to take the easiest path and see where you get stuck.\n\n  \n  \nDiscuss",
      "published_date": "2025-03-12T03:59:22+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "The Social Economy",
      "url": "https://www.lesswrong.com/posts/FLsAA6oKzBTg4qDEs/the-social-economy",
      "author": "kylefurlong",
      "content": "Published on March 11, 2025 10:51 PM GMT  \n  \n\n\nThe Social Economy is a proposal in political economy to equalize purchasing power through income based pricing. Informally called PMI, this new \"currency\" gives everyone about 100 PMI worth of purchasing power each month. Since the average monthly income in the United States is about 4000 dollars, 1 PMI roughly equates to about 40 dollars. Note, though, that a high income earner spending 0.25 PMI on a burrito will spend much more in dollars than someone with no income.  \n  \nThis works for business because at the end of every month, the IRS (or whatever new body administers the PMI transactions) \"evens\" revenue. That is, businesses with higher than average PMI values per transaction transfer some of their base currency revenue to those with lower than average PMI values. This means both that businesses can count on a \"stable\" PMI value per transaction, and serve communities with lower incomes without affecting their revenue.  \n  \nPMI alone creates radical social change. When everyone has equal purchasing power, there is no undue economic leverage anyone can apply to anyone else. To make it work though, there need to be new monetary controls in the economy. Physical cash would no longer be legal tender, and account to account transfers (without annotated material exchanges) would no longer be lawful.  \n  \nHowever, PMI on its own doesn't yield a viable economy. If everyone has the same purchasing power no matter what they do, one key work incentive no longer holds. This is actually part of the point of the system: if one _has_ to work (in a post-scarcity society)[0], that is a form of slavery. However, if everyone were to stop working, even with the radical increases to automation that we expect in the near future, the economy would founder. So to reward the people who do work, we introduce a new currency, contribution points.  \n  \nContribution points accumulate to the earner for their contribution to PMI: If you earn any amount of income, that translates to dollars spent with PMI, and the system awards you with contribution points in the amount of the square root of the contribution.  \n  \nThis new social currency encodes a certain value: the person's contribution to the welfare of everyone in the economy. They can be traded for anything that isn't permanent, reservable, or accumulable. This means that the entire material economy operates on PMI, but there is a new \"ephemeral\" or \"performative\" economy operating in contribution points. This is an area of active discussion as is the whole, but the key point is to allow contribution points to give priority access and unique experiences to high contributors, while ensuring they do not contribute to wealth accumulation and rent seeking.  \n  \nPMI and contribution points give us a fully self-contained currency system that could replace the current system and function well, eliminating poverty. However, these alone wouldn't achieve full economic democracy. The entrenched interests of property holders could still charge rents, and accumulate unearned contribution points. In order to achieve full democracy, two other changes are needed to the current system, and one change to our new system.\n\nFirst, shareholder corporations must transition to worker associations. The reason for this is that shareholder corporations, and generally any ownership-based corporation, act as wealth \"concentrators\", giving a few people the ability to charge rents. If we were to implement the Social Economy only as PMI and contribution points, people with ownership stakes in companies would receive outsized contribution points for their actual labor. This gives us a clear understanding of the bad deals the current system enacts. In order to undo this, we set a date, allowing workers to organize, and negotiate buyouts with former owners through the SEC or another similar body. Typically these will be some form of share to IOU conversion, giving former owners a claim on excess income until paid (within certain bounds, on both sides). Former owners receive excess contribution points for this income, as their compensation for allowing the buyouts to proceed.  \n  \nOnce the Social Economy is up and running with worker associations, former employees now guide every economic decision, extending the democracy of the market to businesses themselves, and making contribution points truly accountable to the actual labor of those earning them. This is so, since with worker control of production, workers set wages for their co-workers, valuing their labor \"fairly\".[1]  \n  \nWorker associations take us most of the way to making contribution points a legible marker of social contribution, but we require one other change to the current system to make them exactly equivalent, and prevent capital accumulation and rent seeking. We must end leases and rents. It makes no sense to end shareholder rent behavior and still allow people to charge interest, either for mortgages or leases, or any other form of payment. The Social Economy transitions mortgages, credit, leases, loans, and all other forms of finance to transfer agreements in PMI.[2] Simultaneously, it converts rental agreements (leases) to mortgages, so that former renters now own their homes and other property.  \n  \nTransfer agreements are formal contracts encoded in the transaction processing system between two parties for some number of payments over some period. The full payment amount of a transfer agreement may not be more than the fair market value of the good or service paid for, plus (or minus) some percentage calculated from the moving average of increase (or decrease) in average PMI value. For example, if the economy were to be on an upward trend, the moving average percentage change of average PMI value over some months might be 5%. So a transfer agreement for a 100 PMI car could be made for 105 PMI.  \n  \nGift funding is a new system that allows people (who now have equal purchasing power) to aggregate their purchasing power into blocks for others to make large purchases with. This fully replaces finance and health insurance, and fully enacts economic democracy. How it works is that people stake some portion of contribution points to start a request for themselves or someone else. The number of contribution points is proportional to the amount of the request in PMI. Once started, others can also stake some amount of contribution points, proportional to the amount of the request, in order to give credence to the validity of the request. Both the original poster and any others staking contribution points this way lose all of them if the request is not fully funded, however, those joining in to validate a truly valid request receive 20% extra contribution points back when funded.  \n  \nOnce the market for funding requests is active in this way, we allow everyone to contribute some amount of PMI to any request they wish. If they do, they receive contribution points according to the square root rule on the average PMI value of the gift. This gives people without an income a way to earn contribution points.  \n  \nWith gift funding active, the Social Economy is mostly complete. We have equalized purchasing power, giving everyone equal footing at the cash register. We have transitioned shareholder corporations to worker associations, ending rent seeking and normalizing labor value. And we have ended finance and interest, replacing them first with transitional transfer agreements, then gift funding.  \n  \nWith all of this active, we now have an economy that is mostly democratic, that is, no one person or group of people can exercise disproportionate control over the means of production, people's taste, or the political narrative. However, this is still not enough to achieve full equality. We still have a competitive market, with worker associations vying for share, driven by the contribution point wish of their workers. This being so, we have merely moved the competitive drive and its vices from former owners to the worker associations themselves.  \n  \nThis is vastly better than the current system for all the outlined reasons, especially that businesses are governed by their workers in aggregate, however, worker associations still may have perverse incentives (perverse being any incentive that causes a drive for inequality). We have not removed the accumulative power of worker associations themselves, merely given control of these mechanisms to democratic bodies formed of the workers doing the labor.  \n  \nIn order to ensure that worker associations don't grow without bound, we legislate that no worker association may hold assets unrelated to their business. Additionally, we do not allow mergers or acquisitions.  \n  \nIn order to control the growth and market behavior of these new entities, we also introduce worker credits.  \n  \nWorker credits function in the following way. Any worker association wishing to hire someone must spend a worker credit. Worker credits are offered on auction every month, and any association with worker points may participate, spending any number of points in any number of bids. To earn points for these auctions, associations participate in gift funding in the same way as anyone else.  \n  \nThis means that the associations that can achieve the most growth are those that contribute the most to the general welfare. It also means that worker associations have a secondary purpose beyond satisfying the market, and that is providing funding for large purchases, and funding the most worthy causes. And in contrast to the current system, this isn't merely virtue signaling: an association's growth depends on being actually social in this way.  \n  \nTo further reward the most social associations, we introduce the Cooperation Index. This is calculated in the following way (remembering that all transactions flow through IRS or another body at this point). First, group all individual association monthly bi-directional payments by payer IDs, then rank by amount. Calculate a cooperation value by the power series E[p_n ^ n] where p_n is the nth greatest trade relationship amount, dividing by worker count. Then, put all association cooperation values in distribution, taking the z-score of a given value as its index.[3]  \n  \nWe use this in the following way. Whenever an association bids for worker credits in auction, (Which they can do in any number and size of bets, the system taking the highest bids one to one with credits) their bid is either multiplied or divided by their Cooperation Index, multiplied if positive, divided if negative.  \n  \nWith all of this, we have an economy based not on accumulation, but on contribution. Both contributing to others through your unique skillful labor, but also through your altruism, both in aggregate as worker associations, and as private citizens.  \n  \nThe reason this matters beyond the humanitarian concerns has to do with AI risk. In nearly all of the disaster scenarios outlined on LW recently, the AI achieves its goal by compromising the economic system to achieve inhuman ends.  \n  \nThe Social Economy removes this ability in two ways. First, all economic activity is guided by worker associations, democratic bodies that determine if their activity is valued. Second, and most importantly, the Social Economy removes survival pressure from economic relationships. This means that no economic coercion is possible, and so no amoral, acquisitive, or disaffected person can be convinced to do something that might harm everyone else.  \n  \nThat's all she wrote. Let me know what you think.  \n  \n\\--  \n  \n[0] Current GDP per capita is 2.46 times the average living wage: https://claude.site/artifacts/4ed7751d-27d2-4631-83ea-6853c3a9ab51  \n[1] There are some incumbent effects, so a possible future system will treat those, however, as a first pass, we take a new step forward into a worker-led economy.  \n[2] This would also be where you transition income tax to a flat transaction fee, replacing income tax with a revenue neutral new tax that functions like a combined VAT and sales tax.  \n[3]  We may also apply this to individuals, to incentivise buying from multiple vendors, and giving a multiplier to contribution points.\n\n  \n  \nDiscuss",
      "published_date": "2025-03-12T07:37:57+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "The Grapes of Hardness",
      "url": "https://www.lesswrong.com/posts/P92SM2snrgmvQjNDR/the-grapes-of-hardness",
      "author": "adamShimi",
      "content": "Published on March 11, 2025 9:01 PM GMT  \n  \n\n\nI have an embarrassing secret: I genuinely believe that _Pierre Bourdieu_, the famous french sociologist, had deep insights worth digging for.\n\nIt's embarrassing for a host of reasons. First, I haven't read or studied much Bourdieu. So this is not even the deep pronouncement of an expert, just the confused intuitions of a dilettante. The second source of embarassement is more sociological: the kind of crowds I frequent, my friends and family, are not really the ones to like stuff like Bourdieu (with a few exceptions). It's too continental, too fuzzy, smells too much of bullshit. It fails to provide clear predictions and commitment for its \"theories\", whatever they are.\n\nDespite these deep-seated counter-arguments, the idea to finally study Bourdieu for real this time bubbles up in my mind, over and over again. Maybe I chance a look upon one of the many Bourdieusian tomes in my antilibrary; maybe I just chain thoughts until they reach the vicinity of the sunken insight I feel from the man himself; maybe I just reread old notes and posts that point to or evoke this obscure intuition.\n\nIf I have to express this thought, at like-share-subscribe-point, my mortification redoubles: it's not even about Bourdieu's theories or subject matter.\n\nI'm not against sociology, it's just not my focal point here. Rather, what I diffusely smell from Bourdieu's own words, and from the shape of sociology itself, is an unmined vein of methodological insights.\n\nHow can I expect powerful methods if I don't know the theories, if even my vague understanding suggest a less than stellar success at predicting, modelling, making sense?\n\nBecause what pulls me in Bourdieu is not the positive tricks, methods, and theories — it's the thoughtful recognition of the raggedness of the epistemological terrain.\n\nIn a way, I see Bourdieu himself not as the prophet of the right method, but as the erudite companion to the methodological difficulties of sociology, and then of social sciences, and then of human-involved endeavours in general.\n\nIt is precisely because sociology is so hard and tricky, because I expect at most fragments of answers and nuggets of insights, not the full breadth of a fitting totalizing theory, that I hope reflections on its practice brim with pearls and treasures. And the little that I've read from him makes me confident Bourdieu is a great fountain of such reflections, in books like \" _Le Métier de Sociologue_\" and \" _Sociologie Générale_\"\n\nOnce again, my shameful admission to not have digged deep enough to find, polish, and present any of these hypothetized jewels, means I do not have any finished example to display.\n\nInstead, the train of thought that led me again to Bourdieu, this time in front of my keyboard, suggests a more general, and so less dependent on my lackluster knowledge, inquiry: what is the pattern here? Why do I feel so much more drawn to this particular author, in this particular field, than many of the alternatives? Why of all people and all domains do I expect Bourdieu's sociological reflections to illuminate the deep recesses of methodological difficulties?\n\nI have a tentative answer in two parts.\n\nOn the side of choosing a field, my pet theory is that different domains have different methodological lessons depending (among other things) on their levels of success.\n\nBy and large, I expect fields where the core questions (be they theoretical as in scientific fields or practical as in artistic, design, engineering fields) are mostly solved, to offer a bounty of tricks, techniques, and languages.\n\nThis is what I see as the positive side of comparative methodology: collecting instances of particular successes, and looking for links: are these two instances the same method? Is there a more general pattern underlying them? How do they differ in implementation details, effectiveness, failure modes, and what does that tell us about the method itself, and its context of applicability?\n\nA concrete example of such a study (which I probably need to write down and publish eventually) is my deep-dive last year into what I call the \"tree reconstruction method\".\n\nThis takes a set of objects (manuscripts, species, languages...) and based on comparing how they differ, reconstructs a genealogical tree for these objects. In addition, the method might help reconstruct an idealized[1] ancestor to some or all the objects.\n\nOne of many revelations from comparing the main instances of this method ( _textual criticism_, _historical linguistics_, _phylogenetics_), is that the likelihood of convergent changes/errors affect the implementation details of the method.\n\nConcretely, in phylogenetics, convergent evolution is quite a rare and big visible thing, which means that all differences can be assumed to have a common ancestor, then from these various potential reconstructed trees, we can use heuristics to detect the few cases of convergent evolution. Whereas in textual criticism, there are a set of errors (called polygenetic) which can really easily occur independently: typos, adaptations to local dialects, adding or removing empty words that don't change the meaning much... These also happen far more frequently that the big monogenetic errors which are actually inherited by copying, and on which the tree reconstruction method relies.\n\nThis means that applying reconstruction software (mostly designed for phylogenetics) to massive amounts of data works as is in phylogenetics, but require massive filtering of polygenetic errors by expert in textual criticism if you don’t want to overindex on polygenetic errors.\n\nIn that way, methods get abstracted and characterized through the comparative study of successful fields.\n\nBut what about unsuccessful fields, or only partially successful fields?\n\nWell, I believe such unlucky domains to be the perfect environment to study methodological problems, as opposed to solutions.\n\nThis is because people tend to try methods that work somewhere everywhere. A difficult enough field that has existed for decades must have some fundamental difficulty, a hard kernel on which the obvious tricks broke their teeth.\n\nMost social sciences, including sociology, are an example here, where they are far from a full theory of their chosen topic, and even worse, provide many contradictory answers to the same question. Same nutrition and many areas of medicine. That even applies to subfields and subquestions within fields: the _P vs NP problem_ is where new fancy methods in complexity theory come to get humbled.\n\nThe take I often hear is that these fields failed because people in them were less smart, less competent, less driven, or some similar insult. Yet given the abyssal success rate of the people coming from the \"smart fields\" (wink wink physics) when tackling these problems from \"bad/stupid fields\", I believe instead that a big part of the explanation lies in the underlying epistemic irregularities.\n\nPut it another way, the structures and scaffolds on which most successful methods rely just disappear when you reach one of the hard domains.\n\nWhich is exactly why such fields, if tilled adequately, can reveal so much about epistemic irregularities.\n\nWhereas the practitioner of a successful field simply applies their trick, method, and language, often without having to reflect, the weary explorer of an inhospitable field must learn, eventually, why all his attempts end in failure.\n\nWhen such reflexion is conducted by a bright and thoughtful individual or community, it reveals deep underlying patterns.\n\nMy favorite example is the set of barriers to the P vs NP question. This problem proved so vexing that complexity theorists actually formalized the existence of at least _three \"barriers\"_ that any answer to the million dollar ( _literally_) question must leap over to stand a chance.[2]\n\nSociology, I expect, is a great hard field to ripe such a harvest. It is old enough that the same bottlenecks have been encountered again and again, it has enough funding and prestige to attract non-trivial numbers of brilliant individuals, and it clearly tackles one instance of the ever-shifting and ever-resisting-methods human domain.\n\nWhich leads me to the second part of my answer: sociology, okay, but why Bourdieu?\n\nHe has the right vibe. That's about it.[3]\n\nIf I try a little more to legibilize my intuitions, he clearly thought long and hard about how sociology works, and why it is hard. After all, he wrote a whole book on how to be a sociologist, and his courses at the College de France start with weeks of him digging into the difficulties of sociology.\n\nMore broadly, he seems to have the right mixture of practicality (actually going in the field to study concrete examples), reflectivity (his obvious philosophical bent), and pedagogy (courses at the College de France like his were meant to educate any french citizen who wanted to get in, which requires a deep levelling down of prerequisites and an explicitation of tacit knowledge).\n\nI might be wrong.\n\nWhen and if I pluck the grapes of sociological hardness ripened by Bourdieu, I might find them sour. Or even more disappointing, flavorless.\n\nFor the moment, they still look ripe to me, though.\n\n  1.  **^**\n\nIt is idealized because it tend to compress variations across both space and time into a single instance, which might therefore never have existed. See _my post on proto-languages_ for one such example.\n\n  2.  **^**\n\nAnother project in my long-list is to find a more general methodological interpretation to these barriers. I swear it exists. Somewhere. Somehow.\n\n  3.  **^**\n\nIf I'm honest, a little bit of this vibe comes from Bourdieu's use and respect for the work of _Gaston Bachelard_, my favorite philosopher of science. But justifying and making palatable Bachelard is beyond my energy and my scope for today.\n\n\n  \n  \nDiscuss",
      "published_date": "2025-03-11T21:01:15+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "Don't over-update on FrontierMath results",
      "url": "https://www.lesswrong.com/posts/9HfJbFy3ZZGzNsspw/don-t-over-update-on-frontiermath-results",
      "author": "David Matolcsi",
      "content": "Published on March 11, 2025 8:44 PM GMT  \n  \n\n\n _(As an employee of the European AI Office, it's important for me to emphasize this point: **The views and opinions of the author expressed herein are personal and do not necessarily reflect those of the European Commission or other EU institutions.** )_\n\nWhen OpenAI first announced that o3 achieved 25% on FrontierMath, I was really freaked out. Next day, I asked Elliot Glazer, EpochAI's lead mathematician and the main developer of FrontierMath, what he thought. He said he was also shocked, and expected o3 to \"crush the IMO\" and get an easy gold, based on the fact that it got 25% on FrontierMath.\n\nIn retrospect, it really looks like we over-updated. While the public can't easily try o3 yet, we have access to o3-mini (high) now, which achieves 20% on FrontierMath given 8 tries, and gets 32% using a Python tool. This seems pretty close to o3's result, as we don't know how much extra affordance o3 had while solving the problems, but based on OpenAI's communication, plausibly it's similar to what o3-mini (high) had when it was using a Python tool.\n\nIn spite of its great scores on FrontierMath, o3-mini (high) is nowhere close to \"crushing the IMO\". To the best of my knowledge, it can't solve a single IMO problem from recent years, and in my experiments it's doing somewhat worse than I was in 9th grade on the high school competitions I participated back then.[1][2][3] Other mathematicians and people with competitive math background whom I asked have similar experience.\n\nThat's still impressive from an LLM, and the pace of progress is admittedly very fast. [4] Nonetheless, it's not what we originally expected when 25% on FrontierMath was announced. What causes the discrepancy?\n\nPart of the story might be that OpenAI elicits the model's capabilities better than I do. It looks like they give it more inference time, they give it tools, and in some experiments they give the AI more than one tries. In contrast, I only experimented in the normal o3-mini (high) chat interface, only gave at most a few tries per problem, and while I experimented with a few different prompts, I might have missed some clever elicitation prompt. \n\nDifferences in elicitation certainly seem to matter. OpenAI claims that _\"On FrontierMath, when prompted to use a Python tool, o3‑mini with high reasoning effort solves over 32% of problems on the first attempt, including more than 28% of the challenging (T3) problems\"_ , while EpochAI's own agent scaffolding only got o3-mini (high) to solve 11% of the problems, and they _\"find that o3-mini (medium) has a much higher solve rate on Tier 1 problems than on Tier 2 and Tier 3\"_ , which is not in line with OpenAI's very similar reported scores for T3 and the full dataset. EpochAI comments: _\"Our results differ from OpenAI's reported o3-mini numbers, possibly due to OpenAI using a more powerful internal scaffold, more test-time compute, or because our eval is on the full FM set. We lack full insight into their methods.\"_\n\nIt's possible that the great elicitation techniques that made OpenAI get 32% on FrontierMath would also make the models much better on my 9th grade competition problems, in which case many of the points I'm making in this post are less true. I'm skeptical though that OpenAI's elicitation would make that big of a difference on my problems, it really looks like the models are going in circles and giving them more inference time after a point doesn't help produce a good proof, and I can't imagine how Python tool use would help. I think it's also suggestive that OpenAI didn't brag about IMO results, which makes it more likely that they can't get good performance on it either. But still, take everything I say from now on with a grain of salt, in case OpenAI's elicitation actually makes a big difference.\n\nI suspect that the main cause of discrepancy between my results and FrontierMath scores is not elicitation, but the different nature of the problems. For the ease of grading, FrontierMath only uses problems where the answer is an integer or another easily describable concrete expression, like a matrix. There are competitions like this for human students too, a notable example being AIME, a test that o3-mini also excels at. I find that competitions like this, where the answer needs to an integer, usually require pretty different skills than \"normal\" competitions that require proofs, or the job of mathematicians. In particular, when I read through the AIME problems, I find that the main idea is almost always obvious once you've seen enough similar problems, and the difficulty comes in correctly doing the calculation in the 12 minutes on average that you can spend on a problem. It is no surprise that AIs do better on AIME than on problems that require proofs that usually need more complex ideas to solve.\n\nWhy can't we make problems where the solution is a number that also require the same creativity that we usually see in problems with proofs? In theory, I don't see a clear obstacle. In practice however, I helped in the problem selection of some math competitions that needed to have numbers as the solution, and I found that it's surprisingly hard to make creative math problems in this format. I heard the same from others working on creating math problems too.\n\nI heard that the creators of FrontierMath tried very hard to create problems that require similar reasoning and creative abilities as normal problems with proofs. I applaud their effort, and I'm sure they managed to create many such interesting problems. Unfortunately, even if they managed to make many of their problems interesting and creative, I suspect that many of their other problems fall into similar uninteresting patterns than the AIME problems or the problems I tried to create for other competitions that needed a number as their solution. At least, that's the only way I can explain how o3-mini (high) (with tool assistance) gets 32% on FrontierMath, while still falling short of me in 9th grade on competition problems with proofs. \n\nIt's also notable that in OpenAI's tests, o3-mini-high achieves 28% of the supposedly challenging T3 problem set on FrontierMath, barely below the average performance. I think this strengthens my belief that FrontierMath doesn't give a really good feel of \"how good a mathematician the AI is\". If it gets similar scores on IMO level and research level problems, then is it as good as a smart high schooler or a researcher? My guess is neither, it's just a similar percentage of problems in the two datasets turned out to be unexpectedly idea-less and simple for the AI.[5]\n\nAll of this makes it very hard to interpret FrontierMath scores for the public who can't see the dataset. If tomorrow o4 came out and achieved 60% score on FrontierMath, how should I update based on that? Did it just improve from 8th grade to 9th grade level in proof-requiring math? Or did o3-mini already burn through most of the problems that turned out to be unexpectedly simple, and the jump from 30% to 60% means that o4 started doing much smarter things and is now at researcher level in proofs? I have no way of knowing, and the only thing I could do is to wait for the new model to be released so I can test it on a few problems myself. \n\nGiven the fact that even the creators of FrontierMath were caught in surprise by the models getting 32% on FrontierMath while being very bad on the IMO, I don't think anyone can reliably predict what the transfer will be between higher scores on FrontierMath and better performance on problems with proofs.\n\n# What is the purpose of benchmarks?\n\nThe reason the AI safety community is interested in creating benchmarks is usually something like this:\n\nThere is an important type of capability, and the AI safety community wants to inform the world [6] about how AIs are progressing in it. This needs to be quantified to be measurable, so they create a benchmark that intends to represent the relevant capability as closely as possible. Then people measure new AIs against the benchmark, and hope that this new information will lead to positive results. \n\nMaybe decision-makers will find it scary when they see how strong the AIs are, and will take AI risk more seriously. If they won't find it scary, at least maybe they will have better awareness of where AIs are now, and it hopefully helps them make better decisions. Maybe there will be red-lines and If-then commitments conditioned on reaching certain capability levels. Maybe if we draw a line through the data-points we have, we can make a reasonable forecast on when the AIs will be so capable that it would require certain actions.\n\nThe story for how the existence of benchmarks leads to good outcomes is more straightforward for dangerous capability evaluations, though some [7] are skeptical even there that it has much of a positive effect.\n\nIt's less clear why it's good from the perspective of AI safety to create a benchmark for a general capability like mathematics, that's not strongly tied to any risk model. It probably accelerates AI improvement by providing a metric to iterate against, while it's unclear which direction a cool result on a math benchmark moves the public discourse. \n\nStill, there is something to be said for the position that being better informed about the world is just good in general, and we should just measure and honestly report all sorts of capabilities. Selfishly, I certainly find it useful to look at AI capabilities on math, as this is the domain I'm most comfortable in, so this is what gives me the most intuitive \"feel\" of how smart the AIs currently are. And even more selfishly, I'd like to know when I should alert my math contest organizer friends that cheating is becoming easier, and when I should alert my mathematician friends that they are going to lose their jobs.\n\nUnfortunately, even if we grant that it's good to be well-informed about how good AIs are at math, FrontierMath basically doesn't help in that. I think I am more or less the perfect target audience for FrontierMath results, and as I said above, I would have no idea how to update on the AIs' math abilities if it came out tomorrow that they are getting 60% on FrontierMath. \n\nUnlike earlier, simpler benchmarks, I can't just look at a few example problems and get a feel of how impressive it is to get X% on the benchmarks. Most of the example problems involve too much background knowledge that I lack, so I can't assess how much reasoning ability it takes to solve them once someone knows the relevant field. \n\nI was already skeptical of the theory of change of \"Mathematicians look at the example problems, get a feel of how hard they are, then tell the world how impressive an X% score is\". But I further updated downward on this when I noticed that the very first public FrontierMath example problem (Artin primitive root conjecture) is just non-sense as stated,[8][9] and apparently no one reported this to the authors before I did a few days ago. Spotting that the problem doesn't make sense took 5 minutes, and the example problems have been publicly available for 4 months now, which puts an upper bound on how many mathematicians scrutinized the public example problems enough to have a good feel of their nature and difficulty. \n\nThis means that I don't have a good intuition of the difficulty of FrontierMath problems, and apparently no one else in the public really does either, so we can't really tell what it means for an AI to do well on the benchmark. And I can't tell my friends how useful o3-mini is for cheating on the competitions they organize based on FrontierMath results, and I certainly can't tell them how much I expect the AIs to help them in their jobs as mathematicians. The transfer between FrontierMath scores and math involving proof (the only type of math that mathematicians actually care about and have some intuitive feeling of)[10], is just too weak and uncertain for that.\n\nThis means that I really don't know what to do with FrontierMath result. I guess it's useful to compare existing models, o3-mini (high) getting higher score than Claude-3.7 probably reflects a real difference in math capabilities, and it can be a useful strategic information to know which companies are currently on top. Still, it's kind of sad if the only use the AI safety community gets out of a benchmark is to use it as a contest among companies.\n\n# How can a benchmark be more informative?\n\nI don't want to single out FrontierMath as unusually bad in this regard. I only write about it because I was originally unusually excited about this benchmark, assembled with great work and care, focusing on a domain that's especially close to me. In the case of some other benchmarks, like Humanity's Last Exam, I have even harder time seeing what it is supposed to inform me about, or how I could update on its results.[11]\n\nHow could we make benchmarks that are more informative to people who don't have a very intimate feel of the data-set and are not extremely knowledgeable about the domain subject?\n\nWhile I have no personal experience creating benchmarks, and I'm fully aware that these things are always easier said than done, here are a few framings I would suggest:\n\n  1.  **Focus on capabilities which have a clear path to danger or dramatic consequences.** If you can demonstrate how much progress AIs make towards making bioweapons or automating the job of OpenAI's engineers, it's pretty straightforward to explain why people should care about these capabilities.\n  2.  **Try to anchor to existing jobs and competitions.** People (at least if they are somewhat socially close to these jobs and competitions) have some intuitive feel of how impressive it is to be an IMO medalist or a Google programmer. So if you can measure the AIs performance on prestigious competitions, or real life tasks from a company's workflow, that can give some idea on how good the AI is and how fast it's progressing.[12] \n  3.  **Report easily understandable metrics, even if the definitions need to be a little handwave-y.**\n    1. I really like work that tries to determine **what's the longest time horizon task on which an AI is competitive with humans**. Even for people who don't have domain expertise and don't have an intuitive feel of the hardness of the example problems, it's easy to look at the trend line and see the pace of progress. Indeed, that's how I usually explain AI progress to my friends.\n    2. Analogies with **age and academic level** are often sloppily conflating knowledge with intelligence, but I still believe it's a reasonable framing of AI development that people can easily understand. That's how I mentally track the AIs' improvements in math: o1 and o3-mini (high) feel about as good as I was in 8th grade, o1-preview was 7th grade, etc.\n    3.  **Human speed-ups** are a great metric though vary a lot between people and tasks. Still, I think AIs are good enough that they can accelerate most professionals in most domains at least a little, so we can start measuring how this acceleration increases, and fit trend-lines to it.\n  4. If possible, **test reasoning and other interesting capabilities with problems requiring minimal background knowledge**. Thus, smart amateurs have an easier time evaluating how impressive the example problems from the dataset are.\n  5.  **In mathematics, use proofs** instead of numerical solutions. It's harder to grade and a little less objective, but I think it should be doable, and it gives better information on the AIs' progress in the type of math that mathematicians actually care about.\n\n\n\nPS: I want to reiterate that I really like Epoch's work in general, and they did a cool job with FrontierMath too, I'm just expressing my sadness that it's hard to gain useful information from the results due to issues that unfortunately affect most other existing benchmarks too.\n\n \n\n  1.  **^**\n\nFor calibration, I later became an IMO silver medalist, so I was pretty good, though it's unlikely I would have made the IMO cut if I lived in the US instead of the much smaller Hungary.\n\n  2.  **^**\n\nI mostly tested the AIs on problems from the Hungarian high school competition, KöMaL, which has a great collection of problems of various difficulty levels, with new ones coming out every month. (I recommend this resource to everyone interested in competitive math, every problem is available in English now.) In my experiments, o3-mini (high) does about as well as the best student of Hungary would do in 8th grade. In contest B, the one that's the appropriate difficulty for current AIs, students have one months to solve 6 problems out of the 8 given that month. The AIs currently solve like 40% of 5 points problems and very few of the hardest 6 points problems. I think in 9th grade I usually spent around an hour on average on 5 points problems, but almost always managed to solve them. \n\n  3.  **^**\n\nAn example of a 5 points problem from KöMaL from this year's December: _\"Jumpy, the grasshopper is jumping around on the positive integers of the number line, visiting each exactly once. Is it possible that the lengths of his jumps produce every positive integer exactly once?\"_ I couldn't make o3-mini (high) solve this problem, however many times I tried, in fact it got even the final answer consistently wrong. In contrast, four 9th graders and one 8th grader student from Hungary solved the problem in December. \n\n  4.  **^**\n\nThough interestingly, in my experiments o3-mini (high) wasn't noticeably smarter than o1. However, both were significantly smarter than o1-preview, and much smarter than GPT-4o.\n\n  5.  **^**\n\nA necessary caveat is that these scores come from OpenAI's tests with unknown methodology, and in EpochAI's tests, the models get significantly worse scores of T3 than on T1. \n\n  6.  **^**\n\nAnd the AI safety community itself\n\n  7.  **^**\n\nI remember reading a quite well-articulated post making the point that evals are useless because we learned that the public and politicians don't care, but I can't find it anywhere. If someone finds it, I will link to it. \n\n  8.  **^**\n\nBy the current definition, $S_x$ is probably an infinite set, which makes the problem non-sense. If you assume they wanted to say that $S_x$ only consists of primes below $x$, the problem starts to have a meaning, but then the truncation at $x$ in the formula becomes superfluous and it becomes $ord_{p,x}(a)=ord_p(a)$, which is not what they intended. When I contacted Olli, he told me that the mistake is that the definition of $d_x$ was supposed to be \n\ndx:=limt→∞|p≤t:p∈Sx||p≤t:p prime|.\n\nThis is really different from what's currently in the text:\n\ndx:=|Sx||p≤x:p prime|,\n\nand I think it's hard to figure out what the intended definition was based on the current problem statement, I certainly couldn't.\n\n  9.  **^**\n\nI don't want to criticize the authors too much here, I know well enough from being problem selector and creator on math competitions that it's easy to make mistakes in the text of a problem. \n\n  10.  **^**\n\nOf course, other engineering-heavy fields care a lot about mathematics that only needs to get the result right and doesn't involve proofs. Math skills relevant to engineering are also interesting to measure, though my impression is that the AIs already excel at every math that's needed for engineering. But I certainly wouldn't try to measure these skills with problems involving Artin's primitive root conjecture. FrontierMath aims to evaluate capabilities relevant to mathematicians, so it should be assessed from the mathematicians' perspective.\n\n  11.  **^**\n\nA friend told me that the point could be that even though it's hard to have a feeling of what HLE is measuring and what the scores mean, we can at least tell the world once it's saturated that \"We tried _really_ hard to make a very hard benchmarks and the AI got a very high score even on that, the age of benchmarks really is over\". Which is fair enough, this can be a useful message to communicate.\n\n  12.  **^**\n\nEven this is tricky though. If you didn't do competitive math yourself, would you know that you should be way, way more impressed by an AI solving IMO level combinatorics problems than IMO level geometry problems? I think it's just really hard to make the correct updates on AI progress based on results in a field you are not intimately familiar with.\n\n\n  \n  \nDiscuss",
      "published_date": "2025-03-11T20:44:04+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "Response to Scott Alexander on Imprisonment",
      "url": "https://www.lesswrong.com/posts/Fp4uftAHEi4M5pfqQ/response-to-scott-alexander-on-imprisonment",
      "author": "Zvi",
      "content": "Published on March 11, 2025 8:40 PM GMT  \n  \n\n\nBack in November 2024, Scott Alexander asked: Do longer prison sentences reduce crime?\n\nAs a marker, before I began reading the post, I put down here: Yes. The claims that locking people up for longer periods after they are caught doing [X] does not reduce the amount of [X] that gets done, for multiple overdetermined reasons, is presumably rather Obvious Nonsense until strong evidence is provided otherwise.\n\nThe potential exception, the reason it might not be Obvious Nonsense, would be if our prisons were so terrible that they net greatly increase the criminality and number of crimes of prisoners once they get out, in a way that grows with the length of the sentence. And that this dwarfs all other effects. This is indeed what Roodman (Scott’s anti-incarceration advocate) claims. Which makes him mostly unique, with the other anti-incarceration advocates being a lot less reasonable.\n\nIn which case, yes, we should make dramatic changes to fix that, rather than arguing over sentence lengths, or otherwise act strategically (e.g. either lock people up for life, or barely lock them up at all, and never do anything in between?) But the response shouldn’t be to primarily say ‘well I guess we should stop locking people up then.’\n\nScott Alexander is of course the person we charge with systematically going through various studies and trying to draw conclusions in these spots. So here we are.\n\n#### Deterrence\n\nFirst up is the deterrence effect.\n\n> Scott Alexander: Rational actors consider the costs and benefits of a strategy before acting. In general, this model has been successfully applied to the decision to commit crime. Studying deterrence is complicated, and usually tries to tease out effects from the _certainty_ , _swiftness_ , and _severity_ of punishment; here we’ll focus on severity.\n\nAccording to every study and analysis I’ve seen, certainty and swiftness matter a lot, and indeed you get more bang for your buck on those than you do on severity past some reasonable point. The question on severity is if we’re reaching decreasing marginal returns.\n\nA bunch of analysis mostly boils down to this:\n\n> I think all four of these studies are consistent with an extra year tacked on to a prison sentence deterring crime by about 1%. All studies start with significant prison sentences, and don’t let us conclude that the same would be true with eg increasing a one day sentence to a year-and-a-day.\n> \n> Helland and Drago et al both suggest that deterrence effects are concentrated upon the least severe crimes. I think this makes sense, since more severe crimes tend to be more driven by emotion or necessity.\n\nI would have predicted a larger effect than this, but it’s not impossible that once you’re already putting someone away for 5+ years you’ve already done most of the deterrence work you’re going to do via sentence length alone – if you thought you’d be caught and cared about your future you wouldn’t be doing it.\n\nThe incarceration effects, on the other hand, naively look rather huge. There’s strong evidence that a few people will constantly go around committing all the crime. If you lock up those doing all the crime, they stop doing the crime, and crime goes down. The math is clear. So why didn’t California’s three strikes law do more work?\n\nIf you credit three strikes with the change in relative crime for five years after the law was passed, you get a 7% drop, although ‘most criminologists suggest that even this is an overestimate, and the true number is close to zero.’\n\nI actually think the 7% estimate looks low here. We see a general trend beforehand of California’s crime rate spiralling out of control, both in absolute and relative terms. It seems likely this trend had to be stalled before it was reversed, and the gap was essentially gone after a while, and other states were also going ‘tough on crime’ during that period, so the baseline isn’t zero.\n\n> So we expected Three Strikes to decrease crime by 83%, but in fact it decreased it by 0-7%. Why?\n> \n> Because California’s Three Strikes law was weaker than it sounds: it only applied to a small fraction of criminals with three convictions. Only a few of the most severe crimes (eg armed robberies) were considered “strikes”, and even then, there was a lot of leeway for lenient judges and prosecutors to downgrade charges. Even though ~80% of criminals had been arrested three times or more, only 1–4% of criminals arrested in California were punished under the Three Strikes law.\n\nWhereas a Netherlands 10-strike (!) law, allowing for much longer sentences after that, did reduce property crime by 25%, and seems like it was highly efficient. This makes a lot of sense to me and also seems highly justified. At some point, if you’re constantly doing all the crime, including property crime, you have to drop the hammer.\n\nWe are often well past that point. As Scott talks about, and this post talks about elsewhere (this was the last section written), the ‘we can’t arrest the 327 shoplifters in NYC who get arrested 20 times per year’ is indeed ‘we suck.’ This isn’t hard. And yes, you can say there’s disconnects where DAs say an arrest is deterrent enough whereas police don’t see a point to arresting someone who will only get released, but that doesn’t explain why we have to keep arresting the same people.\n\nAnalyses from all perspectives, that Scott looks at, agree that criminals as a group tend to commit quite a lot of crime, 7-17 crimes per year.\n\nI also note that I think all the social cost estimates are probably way too low, because they aren’t properly taking into account various equilibrium effects.\n\n#### El Salvador\n\nThat’s what I think happened in El Salvador, that Scott is strangely missing. The reason you got a 95% crime decrease is not some statistical result based on starting with lower incarceration rates. It is because before the arrests, the gangs were running wild, were de facto governments fighting wars while the police were powerless. Afterwards, they weren’t. It wasn’t about thinking on the margin.\n\nWe also get confirmation that theft is way down in El Salvador, in a ‘now I can have a phone in my hand or car on the street and not expect them to be stolen so often I can’t do that’ sense.\n\n#### Roodman on Social Costs of Crime\n\nLater on, Roodman attempts to estimate social costs like this:\n\n> Roodman uses two methods: first, he values a crime at the average damages that courts award to victims, including emotional damages. Second, he values it at what people will pay – how much money would you accept to get assaulted one extra time in your life?\n> \n> These estimates still exclude some intangible costs, like the cost of living in a crime-ridden community, but it’s the best we can do for now.\n\nThese to me seem like they are both vast underestimates. I don’t think we can just say ‘best we can do’ and dismiss the community costs.\n\nI would pay a lot to not be assaulted one time. I’d pay so much more to both not be assaulted, and also not to have the fear of assault living rent free in my head all the time (and for women this has to be vastly worse than that). And for everyone around me to also not having that dominate their thinking and actions.\n\nSo yeah, I find these estimates here rather absurdly low. If we value a life at $12 million when calculating health care interventions, you’re telling me marginal murders only have a social cost of $9.4 million? That’s crazy, murder is considered much worse than other deaths and tends to happen to the young. I think you have to at least double the general life value.\n\nThe rape number is even crazier to me.\n\nHere’s Claude (no, don’t trust this, but it’s a sanity check):\n\n> Claude Sonnet 3.5: Studies estimating the total societal cost per rape (including both tangible and intangible costs) typically range from $150,000 to $450,000 in direct costs. When including long-term impacts and psychological harm, some analyses place the full societal cost at over $1 million per incident.\n> \n> …\n> \n> Total cost estimates per burglary typically range from $3,000 to $7,000 in direct costs, with comprehensive social cost estimates ranging from $10,000-$25,000 per incident when including psychological impact and system costs.\n\nSo yeah, I think even without norm and equilibrium effects these numbers are likely off by a factor of at least 2, and then they’re wrong by a lot again for those reasons.\n\nScott later points out that thinking on the margin gets confusing when different areas have different margins, and in some sense the sum of the margins must be the total effect, but some sort of multiple equilibrium (toy) model seems closer to how I actually think about all this.\n\n#### Recidivism\n\nThe aftereffects of imprisonment forcing or leading people deeper into crime is the actual counterargument. And as Scott points out, it’s crazy to try and claim that the impact here is zero:\n\n> As far as I can tell, most criminologists are confused on this point. They’re going to claim that the sign of aftereffects is around zero, or hard to measure – then triumphantly announce that they’ve proven prison doesn’t prevent crime.\n\nIf the effect here is around zero, one that’s quite the coincidence, and two that would mean prison reduces crime. The actual argument that prison doesn’t reduce crime, that isn’t Obvious Nonsense, is if the aftereffects are very large and very negative.\n\nHere’s one study that definitely didn’t find that.\n\nScott then says there are tons of other studies and it’s all very complicated. There’s lots of weirdness throughout, such as Berger saying everyone pleading guilty means a ‘unusual study population’ despite essentially everyone pleading guilty in our system.\n\nRoodman not only concludes that longer sentences increase crime after, but that harsher ones also do so, while saying that effects at different times and places differ.\n\nAnother suggestion is that perhaps modest sentences (e.g. less than two years) are more relatively disruptive versus incentivizing, and thus those in particular make things worse. That doesn’t seem impossible, but also the incentive effects on the margin here seem pretty huge. You need to be disruptive, or where is the punishment, and thus where is the deterrence? Unless we have a better idea?\n\nGiven the importance of both swiftness and certainty, a strategy of ‘we won’t do much to you until we really do quite a lot to you’ here would be even worse than the three strikes law.\n\nI mean, I can think of punishments people want to avoid, but that aren’t prison and thus won’t cost you your job or family… but we’ve pretty much decided to take all of those off the table?\n\n#### Note on Methodology\n\nIn general, I’ve taken to finding Scott’s ‘let’s look at all the studies’ approach to such questions to be increasingly not how I think about questions at all. Studies aren’t the primary way I look for or consider evidence. They’re one source among many, and emphasizing them this much seems like a cop out more than an attempt to determine what is happening.\n\n#### Conclusions\n\nI do agree broadly with Scott’s conclusions, of:\n\n  1. More incarceration net reduces crime.\n  2. We have more cost-effective crime reduction options available.\n  3. It would be cost effective to spend more on crime reduction.\n\n\n\nTo that I would add:\n\n  1. More incarceration seems net beneficial at current margins, here, because the estimates of social cost of (real, non-victimless) crime are unreasonably low even without equilibrium effects, and also there are large equilibrium effects.\n  2. We have additional even more effective options, but we keep not using them.\n  3. Some of that is ‘not ready for that conversation’ or misplaced ethical concerns.\n  4. Some of that is purely we’re bad at it.\n  5. We should beware medium-sized incarceration periods (e.g. 1-3 years).\n  6. Most importantly: Our current prison system is really bad in that many aspects cause more crime after release rather than less, and the low hanging fruit is fixing this so that it isn’t true.\n\n\n\nAt minimum, we absolutely should be funding the police and courts sufficiently to investigate crimes properly, arrest everyone who does crimes on the regular (while accepting that any given crime may not be caught), and to deal with all the resulting cases.\n\nAnd of course we should adjust the list of crimes, and the punishments, to match that new reality. Otherwise, we are burning down accumulated social capital, and I fear we are doing it rather rapidly.\n\n#### Highlights From Scott’s Comments\n\nScott then followed up with a highlights from the comments post.\n\nIt starts with comments about criminal psychology, which I found both fascinating and depressing. If prospective criminals don’t care about magnitude of risks only certainty of risk and they’re generally not competent to stay on the straight and narrow track and make it work, and they often don’t even see prison as worse than their lives anyway, you don’t have many options.\n\nThe obvious play is to invest bigly in ensuring you reliably catch people, and reduce sentences since the extra time isn’t doing much work, which is consistent with the conclusions above but seems very hard to implement at scale. Perhaps with AI we can move towards that world over time?\n\nI very much appreciated Scott’s response to the first comment, which I’ll quote here:\n\n> Jude: This . . . matches my experience working with some low-income boys as a volunteer. It took me too long to realize how terrible they were at time-discounting and weighing risk. Where I was saying: “this will only hurt a LITTLE but that might RUIN your life,” they heard: “this WILL hurt a little but that MIGHT ruin your life.” And “will” beats “might” every time.\n> \n> One frustrating kid I dealt with drove without a license (after losing it) several times and drove a little drunk occasionally, despite my warnings that he would get himself in a lot of trouble. He wasn’t caught and proudly told me that I was wrong: nothing bad happened, whereas something bad definitely would have happened if he didn’t get home after X party. Surprise surprise: two years later he’s in jail after drunk driving and having multiple violations of driving without a license.\n> \n> Scott Alexander: The “proudly told me that I was wrong – nothing bad happened” reminds me of the Generalized Anti-Caution Argument – “you said we should worry about AI, but then we invented a new generation of large language model, and nothing bad happened!” Sometimes I think the difference between smart people and dumb people is that dumb people make dumb mistakes in Near Mode, and smart people only make them in Far Mode – the smarter you are, the more abstract you go before making the same dumb mistake.\n\nYep. We need to figure out a better answer in these situations. What distinguishes situations where someone can understand ‘any given time you do this is probably net positive but it occasionally is massively terrible so don’t do it’ from ‘this was net positive several times so your warnings are stupid?’\n\nThere was some hopefulness, in this claim that the criminal class does still care about punishment magnitude, and about jail versus prison, as differing in kind – at some point the punishment goes from ‘no big deal’ to very much a big deal, and plea bargains reflect that. Which suggests you either want to enforce the law very consistently, or you want to occasionally go big enough to trigger the break points. But then the next comment says no, the criminals care so little they don’t even know what their punishments would be until they happen.\n\nThese could be different populations, or different interpretations, but mostly this seems like a direct contradiction. None of this is easy.\n\n  \n  \nDiscuss",
      "published_date": "2025-03-11T20:40:06+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "Paths and waystations in AI safety",
      "url": "https://www.lesswrong.com/posts/kBgySGcASWa4FWdD9/paths-and-waystations-in-ai-safety-1",
      "author": "Joe Carlsmith",
      "content": "Published on March 11, 2025 6:52 PM GMT  \n  \n\n\n _(Audio version_ _here_ _(read by the author), or search for \"Joe Carlsmith Audio\" on your podcast app._\n\n _This is the third essay in a series that I’m calling “How do we solve the alignment problem?”.  I’m hoping that the individual essays can be read fairly well on their own, but see __this introduction_ _  for a summary of the essays that have been released thus far, and for a bit more about the series as a whole.)_\n\n# 1\\. Introduction\n\nThe first essay in this series defined the alignment problem; the second tried to clarify when this problem arises. In this essay, I want to lay out a high-level picture of how I think about getting from here either to a solution, or to some acceptable alternative. In particular:\n\n  * I distinguish between the underlying technical parameters relevant to the alignment problem (the “problem profile”) and our civilization’s capacity to respond adequately to a given version of the problem (our “competence profile”).\n  * I lay out a framework for thinking about what competence in this respect consists in – one that highlights the role for three key “security factors,” namely:\n    *  _Safety progress_ : our ability to develop new levels of AI capability safely;\n    *  _Risk evaluation:_ our ability to track and forecast the level of risk that a given sort of AI capability development involves; and\n    *  _Capability restraint_ : our ability to steer and restrain AI capability development when doing so is necessary for maintaining safety.\n  * I distinguish between a number of different possible sources of labor (e.g., both future AI labor, and possible improvements to the quality of future human labor as well) that could improve these security factors.\n  * And I discuss a variety of different intermediate milestones (e.g., global pause, automated alignment research, whole brain emulation, etc) that strategies in this respect could focus on.\n\n\n\nThis high-level picture sets up my next essay – “AI for AI safety” – which argues that we should try extremely hard to use future AI labor to improve the security factors I discuss; and the essay after that, which examines our prospects for safely automating AI alignment research in particular.\n\n# 2\\. Goal states\n\nLet’s start by recalling the goal states we’re aiming for.\n\nIn my first essay, I distinguished between two ways of not-failing on the alignment problem:\n\n  *  **Victory** : avoiding loss of control scenarios while gaining access to the main benefits of superintelligent AI.\n  *  **Costly non-failure** : avoiding loss of control scenarios, but giving up on access to some of the main benefits of superintelligent AI.\n\n\n\nVictory is the ideal. But we should take costly non-failure if necessary.\n\n# 3\\. Problem profile and civilizational competence\n\nWhat does it take to reach either victory, or costly non-failure? I find it useful to decompose the challenge into two components:\n\n  *  **Problem profile** : the settings of the underlying technical parameters relevant to the alignment problem.\n  *  **Competence profile** : the range of problem profiles that our civilization would achieve victory or costly non-failure on.\n\n\n\nHere the rough idea is: the problem profile is uncertain, but it’s not under our control. Rather, it’s set by Nature. Example aspects of the problem profile include: what sorts of training regimes lead to “scheming” by default; the competitiveness hit at stake in preserving human-legible reasoning in AI agents; what sorts of errors in a training signal lead to what sorts of misalignment; etc.\n\nThe competence profile, by contrast, is much more “up to us” – it’s about how our civilization _responds_  to the problem profile we face. I’ll say more below about the factors I think matter most here.[1] \n\nOur overall probability of failure vs. non-failure is determined by the way the problem profile and the competence profile combine – that is, by the probability that we face a given problem profile, multiplied by the probability that we avoid failure on this problem profile conditional on facing it, across all relevant possible problem profiles.\n\nProblem profiles and competence profiles are high-dimensional. But for simplicity, I’ll sometimes talk about a rough spectrum of problem “hardness,” and a rough spectrum of civilizational “competence.”[2] \n\n  * That is, other things equal, harder problem profiles involve the sorts of key issues with motivation control and option control that I’ll discuss later in the series (e.g., issues with adversarial dynamics, opacity, oversight errors, etc) cropping up more consistently and robustly, in a wider range of AI systems, and in manner that requires more effort and resources to resolve.\n  * And more competent civilizations are the ones that avoid failure on such problem profiles nonetheless.\n\n\n\nThe basic goal of action on the alignment problem is to increase civilizational competence on the margin in the highest-expected-value ways. And note, importantly, that this is distinct from trying to ensure safety _regardless of the problem profile_. Indeed: robustness even to worst-case settings of all technical parameters may be quite difficult to achieve, and not the best target for marginal resources.\n\n  * Thus, for example, if you focus too much on making sure that your AI systems are “provably safe,” you may under-invest in more prosaic-but-tractable means of improving their safety.\n  * Or: if you focus too much on ensuring that we avoid loss of control even in scenarios where (a) an algorithmic breakthrough suddenly makes it possible for everyone to turn their old iPhones into a misaligned superintelligence and (b) the offense-defense balance is such that any such superintelligence can easily take over the whole world via a wide variety of methods, then you may end focusing centrally on highly coercive and centralized global governance regimes that come with a multitude of other downsides.\n\n\n\nThat said: I think it is disturbingly plausible that this problem is extremely hard. And I think it’s still well worth trying to improve our prospects in those worlds. Indeed, I am disturbed by how many approaches to the alignment problem seem to imply something like: “and if the problem is hard, then our plan is to die.” Let’s do better.\n\n# 4\\. A toy model of AI safety\n\nIf we’re trying to improve civilizational competence in the highest-expected-value ways, then: what does civilizational competence consist in? That is: what sort of factors determine whether or not we succeed or fail on a given problem profile?\n\nI’m going to focus on three main factors, which I’ll call “security factors.” These are:\n\n  *  **Safety progress** : our ability to develop new levels of AI capability safely.[3] \n\n  *  **Risk evaluation** : our ability to track and forecast the level of risk that a given sort of AI capability development involves.\n  *  **Capability restraint** : our ability to steer and restrain AI capability development when doing so is necessary for maintaining safety.\n\n\n\nWhy focus on these factors? Here’s a highly simplified toy model of AI safety, which will help explain why they stand out to me.[4]\n\nWe can think of the power of frontier AI systems (including both: what capabilities the AIs have, and what sorts of affordances they are given) along a single rough dimension of “AI capability.”[5] And we can think of any given AI developer as having what I’ll call a “capability frontier” (that is, the most powerful AI systems they have developed/deployed thus far), and a “safety range” (that is, the most powerful AI systems that they are able to develop/deploy safely).[6] Thus, in a diagram (and focusing for now on a single actor):[7] \n\nThe main game, here, is to keep the capability frontier within the safety range. And in this context, the three main security factors above serve the following functions:\n\n  *  **Safety progress**   _expands_  the safety range, and makes it _cheaper_ to develop a given level of AI capability safely.[8] \n\n    * Paradigm examples include:\n\n      * progress on controlling an AI’s motivations;\n\n      * restricting AI options for rogue behavior (both via local intervention on its operating environment, and via “hardening” the world more broadly);\n\n      * otherwise designing AI incentives to promote cooperative behavior.\n\n  *  **Risk evaluation**   _tracks_  the safety range and the capability frontier, and it _forecasts_  where a given form of AI development/deployment will put them.\n    * Paradigm examples include:\n      * evals for dangerous capabilities and motivations;\n      * forecasts about where a given sort of development/deployment will lead (e.g., via scaling laws, expert assessments, attempts to apply human and/or AI forecasting to relevant questions, etc);\n      * general improvements to our scientific understanding of AI;[9] \n\n      * structured safety cases and/or cost-benefit analyses that draw on this information.[10]\n\n  *  **Capability restraint**   _steers_  and _pauses_  further development/deployment to keep it within the safety range.\n    * Paradigm examples include:\n      * caution on the part of individual actors;\n      * restrictions on the options for AI development available to a given actor (e.g., via limitations on compute, money, talent, etc);\n      * voluntary coordination aimed at safety (e.g., via mutual commitments, and credible means of verifying those commitments);\n      * enforcement of pro-safety norms and practices (e.g., by governments);\n      * other social incentives and attitudes relevant to decision-making around AI development/deployment (e.g. protests, boycotts, withdrawals of investment, public wariness of AI products, etc).\n\n\n\nAnd to develop superintelligence safely, you’d need to:\n\n  * Make enough safety progress to bring “superintelligence” within the safety range.\n  * Do enough capability restraint (and accurate enough risk evaluation) to refrain from unsafe development in the meantime.[11]\n\n\n\n\nSimple, right? It’s like how: to climb Mount Everest, keep going up, and don’t fall.\n\nThus, on the diagram:\n\nHere’s an analogy. Imagine an expedition into a dark cave. Parts of the cave are covered in toxic slime, which releases a cloud of poison gas if anything touches it. You can remove the slime using a certain kind of heat-lamp, which melts the slime away without releasing the gas. But the slime can be very difficult to see; and currently, you only have a fairly weak flashlight.\n\nHere, loss of control is the slime. Safety progress is the heat-lamp. Risk evaluation is the flashlight. And capability restraint is your ability to not move forward until you’ve melted the slime away.[12]\n\nChatGPT imagines the slime analogy\n\nNow: so far I’ve only been talking about one actor. But AI safety, famously, implicates many actors at once – actors that can have different safety ranges and capability frontiers, and that can make different development/deployment decisions. This means that even if one actor is adequately cautious, and adequately good at risk evaluation, another might not be.[13] And each might worry about the other gaining power/market advantage in a manner that creates incentives for each to take greater risks. And of course, many of these dynamics only get worse as the number of actors increases.\n\nThat said, different actors do not just act in isolation. Rather, their safety progress, risk evaluation, and capability restraint are all influenced by various _collective factors_  – e.g.\n\n  * by publicly available research on motivation control, option control, and incentive design, and by the degree to which the world in general has been hardened against rogue AI attack;\n  * by shared tools and practices for evals, safety cases, forecasting, and other aspects of risk evaluation;\n  * and by the norms, option-restrictions, coordination efforts, enforcement mechanisms, and other incentives that influence a given actor from the outside.\n\n\n\nAnd each actor can themselves _contribute_  to strengthening these collective factors – e.g., by sharing research and best practices, using AI labor for helpful-to-safety applications, participating in coordination and governance schemes, etc.\n\nI’ll define our _civilization’s_  safety progress, risk evaluation, and capability restraint as the factors (individual and collective) that ultimately determine the safety progress, risk evaluation, and capability restraint of each individual actor.\n\nI’ll also throw in one final, catch-all factor, which I’ll call our “ **backdrop capacity**.” By this I mean, roughly, the degree to which our civilization is strong, flourishing, and functional more generally. Paradigm aspects of this functionality include:\n\n  * Backdrop levels of abundance, economic growth, and technological progress.\n  * Baseline levels of health, education, and happiness in the population.\n  * The general quality of our discourse, epistemics, forecasting, and scientific understanding of the world (not just about AI in particular).\n  * The quality, efficiency and trustworthiness of our core institutions and processes (the government, the legal system, etc).\n  * The general availability of high quality cognitive labor and advice.\n  * How well we’re able to coordinate and to resolve conflict productively.\n  * Backdrop levels of various virtues like wisdom, prudence, creativity, integrity, compassion.\n\n\n\nI include “backdrop capacity” because I think it influences the three main security factors above in myriad (albeit, sometimes indirect) ways. I also think that advanced AI might improve our backdrop capacity significantly (though: it could also degrade it in various ways as well). And as in the case of the three main security factors, actors developing/deploying AI can themselves contribute directly to these improvements.\n\nHere’s a diagram of the dynamics I’ve discussed thus far (see footnote for a few of the complexities I’m leaving out[14]):\n\n# 5\\. Sources of labor\n\nI just laid out a toy model of what civilizational competence consists in – a model that focuses on the role of _  safety progress_, _  risk evaluation_, and _capability restraint_ , along with our backdrop capacity more generally. Now I want to talk a little about the different sorts of _labor_  – and especially: cognitive labor – that could improve our civilizational competence in this sense.[15] \n\nWe can divide sources of labor into two categories:\n\n  1. Labor that is available now.\n  2. Labor that might become available in the future.\n\n\n\nThe first category is relatively familiar. It includes, centrally:\n\n  * The labor of current biological humans.\n  * The labor of present-day AI systems.[16]\n\n\n\n\nThe second category is more exotic. It includes, centrally:\n\n  * The labor of _future_ , more advanced AI systems (both AI systems that satisfy the agency prerequisites, and those that do not).\n  * The possibility of what I’ll call “enhanced” human labor – that is, human labor of a significantly higher quality than what we have access to now.\n    * One possible form of “enhanced human labor” is sufficiently high-fidelity human whole brain emulation (WBE) or “uploading” – that is, replication _in silico_  of the cognitively-relevant computational structure of the human brain, in a manner that preserves both human cognitive capabilities _and_  human motivations.\n      * The “preserving human motivations” aspect is important here, because it’s often thought that a key benefit of enhanced human labor is that it will be easier to suitably “align” than future AI labor, because human motivations are more aligned by default.[17] \n\n      * WBE is especially important because it captures some of the _advantages_  of AI labor – i.e., brain emulations can be run very fast, copied in large quantities, etc.\n    * There are also a variety of other possible routes to enhanced human labor – e.g. via biological interventions, or via more external tools (e.g., brain computer interfaces or “BCI”[18]).\n\n      * These come on a spectrum of potency, difficulty, necessary time-lag, and so on.[19] To the extent they rely on biological human brains, though, they don’t share the advantages of WBE and AI labor re: speed, copying, etc.\n\n      * As in the case of WBE, though, a key distinguishing feature of this labor is that it inherits human motivations by default.[20]\n\n    * Obviously, in many cases, creating and drawing on these possible future forms of enhanced human labor would raise serious ethical and political questions – both about how the enhanced humans involved are being treated, and about the implications and consequences for everyone else.[21] But I’m not going to focus on these questions here.\n\n\n\n\nThus, in a diagram:\n\n# 6\\. Waystations on the path\n\nOK, I’ve now discussed the end states we’re shooting for, the security factors that can help us get there, and the sources of labor we can use to improve these security factors. I want to close this essay by discussing a few different intermediate “waystations” that strategies in this respect can focus on[22] – that is, milestones _other than_  one of the end-states we’re aiming for, but which can function nevertheless as key goals for the purposes of current planning.[23] \n\nOne example “waystation” that has already received some attention is some kind of global pause/halt on frontier AI development (we can think of this as a particular form of capability restraint).[24] \n\nAnother waystation would be something like: access to a given form of enhanced human labor. Indeed, my understanding is that the strategy favored by leadership at the Machine Intelligence Research Institute involves going to “Global pause” as a step 1, and then to “Enhanced human labor” as a step 2.[25] (And insofar as developing a given form of enhanced human labor – e.g., whole brain emulation – might take a long time, such a pause could well be necessary.)\n\nA further possible waystation would be: safe access to the benefits of some kind of future AI labor, short of full-blown superintelligence. This sort of strategy can take a variety of different forms, but a paradigm example is a focus on “automated alignment researchers” that then help us significantly with aligning more advanced systems.[26]\n\nNote, though, that “automated alignment researcher” isn’t the only type of future AI labor one can focus on as a waystation. Rather, one could in principle focus on future AI labor that helps with some other application – for example, a narrow “scientist AI” that helps with access to some form of enhanced human labor like whole brain emulation.[27]\n\nAnother example would be: access to AI labor that helps significantly in making a global pause possible – for example, by significantly facilitating risk evaluation, coordination, negotiation, monitoring, enforcement, and so on.[28] (Though: note that to the extent that we are in principle in a position to institute a global pause on AI _now_ , and the main bottleneck is political will rather than technological capacity, it’s not clear exactly how much AI labor on its own will help.[29])\n\nAnd we can imagine a variety of other possible waystations as well. Examples include:\n\n  * Some new ability to formally verify the safety properties of our AI systems.[30] \n\n  * Some large amount of progress on interpretability/transparency, perhaps via some new paradigm of AI development.[31] \n\n  * The formation and empowerment of some suitably safety-concerned global coalition.[32] \n\n  * A regime of “mutually-assured AI malfunction,” in which capability restraint is enforced by the threat (and/or actuality) of different actors sabotaging each other’s projects.\n  * A global moratorium on particular _kinds_  of AI development – e.g., autonomous general agents, as opposed to more tool-like AIs.[33]\n\n  * Radical improvements to our ability to understand and forecast AI safety risks, perhaps via vastly better evals, scary demos, forecasting ability, etc.[34] \n\n\n\n\nOf course, in all of these cases, there’s still a question of where, exactly, one goes from the relevant milestone. That is: if one succeeds in e.g. getting a global pause, or in creating a safe automated alignment researcher, or in developing whole brain emulation technology, one still needs to plot an overall path to victory (or to costly non-failure). Generally, though, a good waystation makes it clear why the path becomes much easier from there.\n\nWe also don’t need to focus on any one milestone in particular. Rather, we can work towards many at once. Indeed, in my next essay – “AI for AI safety” – I’ll defend the crucial importance of trying as hard as we can to use future AI labor, in particular, to strengthen our civilizational competence across the board. I’m especially interested, though, in “automated alignment researcher” as a milestone – and in the essay after next, I’ll analyze it in detail.\n\n  1.  **^**\n\n Though obviously, there are limits to the control that a given “we” can exert in this respect.\n\n  2.  **^**\n\n This simplification does have costs, though, in that strategies that work/don’t-work for one problem profile (for example: focusing a lot on interpretability) might be importantly different in their efficacy for some other problem profile.\n\n  3.  **^**\n\n I’m going to focus on safety from full-blown loss of control risk, but in principle this model extends both to lower-stakes forms of rogue behavior, and to safety from other risks that increase with greater AI capabilities.\n\n  4.  **^**\n\n I’ll also draw on it elsewhere in the series.\n\n  5.  **^**\n\n Obviously, AI capability is extremely multi-dimensional. But the model I’m using extends quite naturally to a multi-dimensional setting as well – the point is to avoid unsafe regions of capability space.\n\n  6.  **^**\n\n Deploying safely isn’t a binary, but I think the model works regardless of the safety standard you fix (via e.g. a given probability of failure).\n\n  7.  **^**\n\n Note that there’s a lot of room for debate about how to draw the lines on a diagram like this. E.g., if you think that the problem profile is easy, then you might put the red line quite high up – or perhaps, if you think that AI development is safe by default, you might not have a red line at all. And there’s a further question about how you think the red line has been changing over time. My particular version of the diagram isn’t meant to be making any claims about this – it’s just illustrative.\n\n  8.  **^**\n\n A more fine-grained model would distinguish between safety in the sense of avoiding loss-of-control, and elicitation in the sense I discussed here. But I’m going to skip over this for now.\n\n  9.  **^**\n\n Including: understanding relevant to which paradigms of AI development pose more or less risk.\n\n  10.  **^**\n\n This overlaps notably with safety progress. And no surprise: making systems safe and understanding _when_  they’re safe vs. dangerous are closely tied.\n\n  11.  **^**\n\n And also: to steer towards safe approaches to superintelligence once available.\n\nAlternatively: _avoiding_  the alignment problem requires exerting enough capability restraint to _not_  build superintelligent AI agents at all.\n\n  12.  **^**\n\n And/or: to steer around slime-y patches.\n\n  13.  **^**\n\n See e.g. Karnofsky (2022) on “inaction risk.”\n\n  14.  **^**\n\n Complexities I’m leaving out (or not making super salient) include: the multi-dimensionality of both the capability frontier and the safety range; the distinction between safety and elicitation; the distinction between development and deployment; the fact that even once an actor “can” develop a given type of AI capability safely, they can still choose an unsafe mode of development regardless; differing probabilities of risk (as opposed to just a single safety range); differing severities of rogue behavior (as opposed to just a single threshold for loss of control); the potential interactions between the risks created by different actors; the specific standards at stake in being “able” to do something safely; etc.\n\n  15.  **^**\n\n Labor isn’t the only relevant input here. But because AI, and the technological advancement it unlocks, might make new and more powerful sorts of labor available, I think it’s an especially important input to track.\n\n  16.  **^**\n\n Maybe it sounds a bit strange to call this “labor,” but I think it’s roughly the right term, and it will be increasingly the right term as more advanced AI systems come online.\n\n  17.  **^**\n\n Indeed, it’s centrally _because_ suitably high-fidelity human brain emulations inherit human motivations by default that I’m counting them as “enhanced human labor” rather than “future AI labor,” despite the fact that they run _in silico_. However, if a brain emulation process isn’t high enough fidelity to preserve human motivations/values, then I’m going to count it as “future AI labor” rather than enhanced human labor. And note that you also need the ems to suitably _retain_  their human-like motivations over time (thanks to Jason Schukraft for discussion). And note, also, that depending on the problem profile, it may end up being _easier_  to shape an AI’s motivations desirably than to create a human brain emulation with the right motivational profile, at least along many dimensions. In particular: the motivations of our AIs are shaped, by default, by much more active human effort. (Thanks to Owen Cotton-Barratt and Will MacAskill for discussion.)\n\n  18.  **^**\n\n Though: I doubt that just allowing direct brain control over external devices is an especially important intervention here, as I don’t think that “ability to affect external devices” is an especially key bottleneck to the quality of human cognitive labor at the moment.\n\n  19.  **^**\n\n And in some cases, the line between “enhanced human labor” and “normal human labor with some new tools” gets blurry.\n\n  20.  **^**\n\n And if it _doesn’t do that_ (i.e., because a given form of intervention warps human values too much), it becomes much less useful.\n\nI’ll also note some general open question about how far we should expect “this agent has human motivations” to go with respect to ensuring the sort of safety from AI takeover that we’re hoping for. That is: it’s sometimes thought that the central concern about the motivations of AI systems is that these motivations will be in some sense _alien_  or non-human. And using enhanced human labor instead is supposed to provide comfort in this respect. But as I’ve discussed in other work (see here, and here), the basic conceptual argument for expecting instrumentally convergent power-seeking, including takeover-seeking, applies with similar force to human-like motivations. And our degree of actual lived comfort with human motivations stems, I think, in central part from the _option control_  that our civilization exerts with respect to individual agents, rather than from any kind of historical precedent where humans routinely refuse takeover options once such options are available and reasonably likely to lead to success. So to the extent that the agents at stake in “enhanced human labor” would end up having takeover options similar to those that we’re expecting advanced AI systems to have, I think we should not assume that enhanced human labor is safe.\n\nIndeed, certain kinds of blueprints for AI motivations – in particular, blueprints that involve the AI systems functioning centrally as an instrument of someone else’s will, and being generally extremely docile/pliant/ready-to-be-shut-down-or-modified etc – seem like a relatively poor fit with human motivations. Or to put it another way, humans may (or may not) be reasonably “nice” by default; but they aren’t especially “corrigible.”\n\n  21.  **^**\n\n Indeed, as I discussed earlier in the series, I think that creating and using advanced AI labor raises these questions too.\n\n  22.  **^**\n\n These waystations generally fall under the umbrella either of one of the security factors I’ve discussed above, or of one of the possible sources of future labor.\n\n  23.  **^**\n\n Indeed, in some sense, my whole framing of the alignment problem is already focusing on a type of “waystation,” in that I’m not trying to plan all the way through to a good future in general – nor, even, to safe scaling up to arbitrary levels of superintelligence. That is, I’m treating something like “safe access to the benefits of some minimal form of full-blown superintelligence” as an end point past which my own planning, with respect to this problem, need not extend.\n\n  24.  **^**\n\n This is a waystation that has been advocated for by the Machine Intelligence Research Institute (MIRI), for example, and by Miotti et al in “A Narrow Path.”\n\n  25.  **^**\n\n From their January 2024 Strategy Update: “Nate and Eliezer both believe that humanity should not be attempting technical alignment at its current level of cognitive ability, and should instead pursue human cognitive enhancement (e.g., via uploading), and then having smarter (trans)humans figure out alignment.” And see also this podcast with Yudkowsky, where he discusses this strategy in more detail.\n\n  26.  **^**\n\n This, for example, was the waystation that OpenAI’s old “superalignment” team was focusing on – see e.g. discussion here: “Our goal is to build a roughly human-level automated alignment researcher⁠. We can then use vast amounts of compute to scale our efforts, and iteratively align superintelligence.” Something in the vicinity is suggested by Sam Bowman’s description of Anthropic’s theory of victory on AI as well (see especially section 2 on “TAI, or Making the AI do our homework”). And see also Clymer (2025) for some more detailed discussion.\n\n  27.  **^**\n\n Indeed, in a sense, we can think of approaches that try to identify some “pivotal act” – i.e., an action that drastically improves the situation with respect to AI safety – that we use AI systems to perform as a generalized version of “Future AI labor” as a waystation. That is, first one gets access to some kind of pivotally useful AI system, and then one “saves the world” from there. See e.g. the strategy outlined in MIRI’s 2017 fundraiser update, and Yudkowsky’s discussion here of “So far as I'm concerned, if you can get a powerful AGI that carries out some pivotal superhuman engineering task, with a less than fifty percent chance of killing more than one billion people, I'll take it.”\n\nHowever: I have a number of problems with “pivotal acts” as a frame. For one thing: I think they often bring in an implicit assumption that the default outcome is doom (hence the need for the world to be “saved”) – an assumption I don’t take for granted. But more importantly: thinking in terms of discrete “pivotal acts” can mislead us about the nature and difficulty of the improvements to our civilizational security factors required in order for the world in question to get “saved.” In particular: those improvements can result from a very large assortment of individual actions by a very large number of different agents, no individual one of which needs to be pivotal. This is a point from Christiano (2022): “No particular act needs to be pivotal in order to greatly reduce the risk from unaligned AI, and the search for single pivotal acts leads to unrealistic stories of the future and unrealistic pictures of what AI labs should do.” See also Critch (2022) here.\n\n  28.  **^**\n\n Some versions of this get very scary very fast.\n\n  29.  **^**\n\n That said: better understanding of the risks, and better ability to identify, rationally-assess, and enforce mutually-acceptable agreements, seems like it could at least make a meaningful difference. Thanks to Nate Soares and Owen Cotton-Barratt for discussion here.\n\n  30.  **^**\n\n We might view the “Guaranteed safe AI” agenda, and related approaches, as focusing on waystations of this kind.\n\n  31.  **^**\n\n MIRI’s 2017 Fundraiser is suggestive of this kind of focal point; and see also Anthropic’s “Interpretability dreams” for some other discussion (though: not framed as a general strategy re: AI safety).  \n\n  32.  **^**\n\n The “entente” strategy that Amodei discusses here might count as an instance of this.\n\n  33.  **^**\n\n See e.g. Aguirre’s “Keep the Future Human” for a tool-AI-focused proposal in this broad vein.\n\n  34.  **^**\n\n See Finnveden on “AI for epistemics” for some relevant discussion – though this discussion isn’t framed specifically as an overall strategy.\n\n\n  \n  \nDiscuss",
      "published_date": "2025-03-11T18:52:57+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    },
    {
      "title": "Meridian Cambridge Visiting Researcher Programme: Turn AI safety ideas into funded projects in one week!",
      "url": "https://www.lesswrong.com/posts/ZuADxSHmTECTteD86/meridian-cambridge-visiting-researcher-programme-turn-ai",
      "author": "Meridian Cambridge",
      "content": "Published on March 11, 2025 5:46 PM GMT  \n  \n\n\nCrossposted from this EA Forum post: https://forum.effectivealtruism.org/posts/W4TppzBnHHHZHwgvi/meridian-cambridge-visiting-researcher-programme-turn-ai  \n  \n **April 6-12, 2025** | **Cambridge, UK**  \n **Apply Now** (Deadline: March 19, 2025, anywhere on Earth)\n\nWe're excited to announce Meridian and Cambridge AI Safety Hub’s Visiting Researcher Programme, running from **April 6-12, 2025, in Cambridge, UK**. This programme offers the opportunity for researchers to transform their AI safety research ideas into robust, fundable proposals — and fast.  \n \n\n##  **What makes this programme different?**\n\nIf you’ve completed a programme like MATS, MARS, ARENA, ML4G, or ERA, you likely know a lot about AI safety. But what next? Many researchers hit a wall between gaining knowledge and launching full-time research projects. That’s the gap we’re aiming to fill.  \n  \nThis programme offers a structured sprint to transform your research ideas into  **submission-ready proposals** for  _Open Philanthropy’s  _ ** _$40 million AI safety research initiative_**. Unlike typical upskilling programs, our focus is on  **team formation** ,  **project development** , and  **grant writing** — not teaching technical skills. By the end of the week, you’ll have a  **polished proposal** and  **targeted, actionable feedback** from experts in the field.  \n \n\n###  **Full Support Provided:**\n\n  * Travel expense reimbursement to and from Cambridge\n  * Accommodation during the programme week\n  * Workspace, resources, and meals during programme activities\n\n\n\nResearchers who secure funding will be **invited back to Cambridge to continue their work at Meridian** , with visa and accommodation support (subject to approval and funding availability). We’ll also provide research management and administrative assistance so you can focus on what matters: your research.\n\n##  **Programme breakdown:**\n\n  *  **Days 1-2:** Explore research directions and **meet potential collaborators** through introductory sessions, workshops, and our speed cofounder matching process. You'll connect with researchers whose skills complement yours and begin developing project ideas together.\n  *  **Day 3:** Form your research team and **refine your project focus**. By the end of day three, you'll have a clear concept and research direction with your newly-formed team.\n  *  **Days 4-5:** Learn effective grant writing strategies from experts and **develop your proposal with structured feedback**. Sessions will cover research impact assessment and alignment with funder priorities.\n  *  **Day 6:** Present your proposal to peers and experts who will offer feedback and **help you strengthen your proposal** 's technical content.\n  *  **Day 7:** Polish your submission with final technical reviews. By day's end, you'll have a completed proposal ready for the April 15th deadline.\n\n\n\n##  **Who Should Apply?**\n\n  * Researchers with expertise in machine learning, computer science, mathematics, or related fields\n  * Graduates of programmes such as MATS, MARS, ARENA, ML4G, SPAR, and ERA with AI safety knowledge looking to transition to full-time research\n  * Individuals with established research portfolios interested in transitioning to AI safety\n  * PhD candidates, recent graduates, or postdoctoral researchers exploring AI safety directions\n  * Principal Investigators and professors interested in incorporating AI safety into their research agenda\n  * People based in Cambridge or willing/excited to work there and contribute to strengthening Cambridge as a hub for AI safety research\n\n\n\n##  **Apply Now**\n\nThe deadline to apply is March 19, 2025 anywhere on Earth. We’re limiting participation to around 30 researchers, so apply early. All the details and the application form are available here:  _Meridian Visiting Researcher Programme_.\n\nWe look forward to seeing your ideas take shape!\n\nThe Meridian Team\n\n  \n  \nDiscuss",
      "published_date": "2025-03-12T00:42:58+00:00",
      "source": "LessWrong",
      "summary": "Recently, AI safety researchers have been talking about important ideas like how to make AI systems safer while still allowing them to learn and develop properly. They're discussing the balance between control and natural safety, kind of like how we need to find the right balance between protecting kids and letting them learn from experience."
    }
  ]
}