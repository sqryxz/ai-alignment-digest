================================================================================
AI Alignment Daily Digest - 2025-03-13

Today's digest contains 9 unique posts across 8 topics.
Some posts appear in multiple categories based on their content.

Key Themes:
-----------
• Technical Progress
  Technical developments in AI systems and capabilities
  6 related posts, including:
  - The Most Forbidden Technique
  - Field Testing in Brazilian Almost Military Training
  - You don't actually need a physical multiverse to explain anthropic fine-tuning.

• AI Safety & Control
  Discussion of AI safety mechanisms, alignment strategies, and risk mitigation
  4 related posts, including:
  - The Most Forbidden Technique
  - Existing UDTs test the limits of Bayesianism (and consistency)
  - The Social Economy

• Research & Methods
  Research methodologies and experimental findings
  3 related posts, including:
  - The Most Forbidden Technique
  - Existing UDTs test the limits of Bayesianism (and consistency)
  - (Anti)Aging 101

• Philosophical Implications
  Exploration of ethical and philosophical aspects of AI development
  3 related posts, including:
  - Existing UDTs test the limits of Bayesianism (and consistency)
  - The Social Economy
  - The Most Forbidden Technique

• Policy & Governance
  Regulatory frameworks and governance approaches
  2 related posts, including:
  - The Most Forbidden Technique
  - Existing UDTs test the limits of Bayesianism (and consistency)


Detailed Posts by Category:
------------------------

=== AI Capabilities (6 posts) ===

• Field Testing in Brazilian Almost Military Training
  By P. João (LessWrong) - 2025-03-12
  Also appears in: Meta
  Publishing stories like this makes me feel more naked than if I were doing a striptease. And the worst part is that no one puts dollars in my underwear, but here it goes: When I decided to write for the LessWrong community about the mechanisms of humor, I thought: "If I managed to avoid jail by using acid comedy in military barracks, I can certainly contribute something useful here. And here's an application." Pre-Haiti Context I received the most Brazilian mission possible: Teach emergency first aid to an entire regiment of the Brazilian army "So it's for a peace mission? Will I be able to do this alone?" "Relax, soldier, you have a week to train them" The standard is 3 intensive months. My solution? Condense intensive care to "priority life" principles: E.g., Broken spine vs. Cardiac arrest - What good is a spine to someone who isn't breathing? And highlight this wisdom with a principle even soldiers couldn't forget: "What's a fart to someone who's already crapped themselves?" Day 1 - The Bathroom Revolution After the fart principle, the class turned into a minefield.

• The Most Forbidden Technique
  By Zvi (LessWrong) - 2025-03-12
  Also appears in: AI Safety, Science, Economics, Technology
  The Most Forbidden Technique is training an AI using interpretability techniques. An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that. You train on [X]. Only [X]. Never [M], never [T]. Why? Because [T] is how you figure out when the model is misbehaving. If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on. Those bits of optimization pressure from [T] are precious. Use them wisely. Table of Contents 1. New Paper Warns Against the Most Forbidden Technique. 2. Reward Hacking Is The Default. 3. Using CoT to Detect Reward Hacking Is Most Forbidden Technique. 4. Not Using the Most Forbidden Technique Is Harder Than It Looks. 5. It’s You, It’s Also the Incentives. 6. The Most Forbidden Technique Quickly Backfires. 7. Focus Only On What Matters. 8. Is There a Better Way? 9.

• You don't actually need a physical multiverse to explain anthropic fine-tuning.
  By Fraser (LessWrong) - 2025-03-12
  Also appears in: AI Safety, Science
  The standard argument usually goes something like this: > Alice: there's a lot about the Earth that seems suspiciously fine-tuned for the evolution of complex life. > > Our orbit happens to lie in this fairly narrow band that allows for liquid water, and it seems other solvents don't work nearly as well in supporting biochemistry. > We've lucked into an ozone layer and a strong magnetic field, shielding us from cosmic rays and solar radiation. > Our moon is far larger than we'd expect for a planet our size - to the extent that it might be more correct to classify the Earth-Moon system as a double planet. It's plausible that the lunar-powered periodic cycle of tidal conditions could have been crucial as a sort of temperature input in the simulated annealing process of evolution - very simple local optima are less likely be stable and dominant in a dynamic environment. > We've been dealt a perfect hand of accessible elements - H, C, N, O, P, and S - and we're unique among the terrestrial planets in our active plate tectonics to recirculate and buffer stuff that would otherwise build up or deplete from the biosphere.

• AI Can't Write Good Fiction
  By JustisMills (LessWrong) - 2025-03-12
  Also appears in: AI Safety
  When Deepseek came out, there was a lot of fanfare about it being good at creative writing. I like AI and I love creative writing, so I decided to give it a spin. Specifically, I told Deepseek to do its best to write a story that might get accepted at Smokelong, one of the best flash fiction magazines in the business. It came up with: > The morning her shadow began unspooling from her feet, Clara found it coiled beneath the kitchen table like a serpent made of smoke. It didn’t mirror her anymore—not the tremble in her hands as she poured coffee, not the way she pressed a palm to her ribs, as if holding herself together. It just watched. > > “You’re not him,” she whispered, but the shadow rippled, ink-dark edges softening into a silhouette too broad, too familiar. She’d buried that shape six months ago, shoveled dirt over its echo. Yet here it was, pooling in the cracks of the linoleum. The “story” continued from there, but you probably get the idea. Superficially, the pieces are there. Evocative imagery, a dark emotional theme, sensory metaphors.

• Existing UDTs test the limits of Bayesianism (and consistency)
  By Cole Wyeth (LessWrong) - 2025-03-12
  Also appears in: AI Safety, Science, Economics, Meta, Philosophy
  Epistemic status: Using UDT as a case study for the tools developed in my meta-theory of rationality sequence so far, which means all previous posts are prerequisites. This post is the result of conversations with many people at the CMU agent foundations conference, including particularly Daniel A. Herrmann, Ayden Mohensi, Scott Garrabrant, and Abram Demski. I am a bit of an outsider to the development of UDT and logical induction, though I've worked on pretty closely related things. I'd like to discuss the limits of consistency as an optimality standard for rational agents. A lot of fascinating discourse and useful techniques have been built around it, but I think that it can be in tension with learning at the extremes. Updateless decision theory (UDT) is one of those extremes; but in order to think about it properly, we need to start with its Bayesian roots. Because, appropriately enough for a sequence on the meta-theory of rationality, I want to psychoanalyze the invention/inventors of UDT. Hopefully, we'll then be in a position to ask what we think we know and how we think we know it in regards to updatelessness (also sometimes called priorism), the driving idea behind UDT.

• The Social Economy
  By kylefurlong (LessWrong) - 2025-03-12
  Also appears in: Economics
  The Social Economy is a proposal in political economy to equalize purchasing power through income based pricing. Informally called PMI, this new "currency" gives everyone about 100 PMI worth of purchasing power each month. Since the average monthly income in the United States is about 4000 dollars, 1 PMI roughly equates to about 40 dollars. Note, though, that a high income earner spending 0.25 PMI on a burrito will spend much more in dollars than someone with no income. This works for business because at the end of every month, the IRS (or whatever new body administers the PMI transactions) "evens" revenue. That is, businesses with higher than average PMI values per transaction transfer some of their base currency revenue to those with lower than average PMI values. This means both that businesses can count on a "stable" PMI value per transaction, and serve communities with lower incomes without affecting their revenue. PMI alone creates radical social change. When everyone has equal purchasing power, there is no undue economic leverage anyone can apply to anyone else. To make it work though, there need to be new monetary controls in the economy.

=== AI Safety (6 posts) ===

• Many life-saving drugs fail for lack of funding. But there’s a solution: desperate rich people
  By Mvolz (LessWrong) - 2025-03-12
  Two scrappy middle-aged men without PhD.s (self-described) have ethical solutions on how to fund drug trials with potential, including an anti-aging drug. Discuss.

• The Most Forbidden Technique
  By Zvi (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, Science, Economics, Technology
  The Most Forbidden Technique is training an AI using interpretability techniques. An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that. You train on [X]. Only [X]. Never [M], never [T]. Why? Because [T] is how you figure out when the model is misbehaving. If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on. Those bits of optimization pressure from [T] are precious. Use them wisely. Table of Contents 1. New Paper Warns Against the Most Forbidden Technique. 2. Reward Hacking Is The Default. 3. Using CoT to Detect Reward Hacking Is Most Forbidden Technique. 4. Not Using the Most Forbidden Technique Is Harder Than It Looks. 5. It’s You, It’s Also the Incentives. 6. The Most Forbidden Technique Quickly Backfires. 7. Focus Only On What Matters. 8. Is There a Better Way? 9.

• You don't actually need a physical multiverse to explain anthropic fine-tuning.
  By Fraser (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, Science
  The standard argument usually goes something like this: > Alice: there's a lot about the Earth that seems suspiciously fine-tuned for the evolution of complex life. > > Our orbit happens to lie in this fairly narrow band that allows for liquid water, and it seems other solvents don't work nearly as well in supporting biochemistry. > We've lucked into an ozone layer and a strong magnetic field, shielding us from cosmic rays and solar radiation. > Our moon is far larger than we'd expect for a planet our size - to the extent that it might be more correct to classify the Earth-Moon system as a double planet. It's plausible that the lunar-powered periodic cycle of tidal conditions could have been crucial as a sort of temperature input in the simulated annealing process of evolution - very simple local optima are less likely be stable and dominant in a dynamic environment. > We've been dealt a perfect hand of accessible elements - H, C, N, O, P, and S - and we're unique among the terrestrial planets in our active plate tectonics to recirculate and buffer stuff that would otherwise build up or deplete from the biosphere.

• AI Can't Write Good Fiction
  By JustisMills (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities
  When Deepseek came out, there was a lot of fanfare about it being good at creative writing. I like AI and I love creative writing, so I decided to give it a spin. Specifically, I told Deepseek to do its best to write a story that might get accepted at Smokelong, one of the best flash fiction magazines in the business. It came up with: > The morning her shadow began unspooling from her feet, Clara found it coiled beneath the kitchen table like a serpent made of smoke. It didn’t mirror her anymore—not the tremble in her hands as she poured coffee, not the way she pressed a palm to her ribs, as if holding herself together. It just watched. > > “You’re not him,” she whispered, but the shadow rippled, ink-dark edges softening into a silhouette too broad, too familiar. She’d buried that shape six months ago, shoveled dirt over its echo. Yet here it was, pooling in the cracks of the linoleum. The “story” continued from there, but you probably get the idea. Superficially, the pieces are there. Evocative imagery, a dark emotional theme, sensory metaphors.

• Existing UDTs test the limits of Bayesianism (and consistency)
  By Cole Wyeth (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, Science, Economics, Meta, Philosophy
  Epistemic status: Using UDT as a case study for the tools developed in my meta-theory of rationality sequence so far, which means all previous posts are prerequisites. This post is the result of conversations with many people at the CMU agent foundations conference, including particularly Daniel A. Herrmann, Ayden Mohensi, Scott Garrabrant, and Abram Demski. I am a bit of an outsider to the development of UDT and logical induction, though I've worked on pretty closely related things. I'd like to discuss the limits of consistency as an optimality standard for rational agents. A lot of fascinating discourse and useful techniques have been built around it, but I think that it can be in tension with learning at the extremes. Updateless decision theory (UDT) is one of those extremes; but in order to think about it properly, we need to start with its Bayesian roots. Because, appropriately enough for a sequence on the meta-theory of rationality, I want to psychoanalyze the invention/inventors of UDT. Hopefully, we'll then be in a position to ask what we think we know and how we think we know it in regards to updatelessness (also sometimes called priorism), the driving idea behind UDT.

• (Anti)Aging 101
  By George3d6 (LessWrong) - 2025-03-12
  Also appears in: Science
  A quest to solve aging must start with careful consideration of what it is. Aging is a constant in nature, from archaea to elephants. Where life finds death as counterpart, aging is unopposed, yet frames our understanding of both. To solve aging is to prevent death and avoid decay. Death is a rather sudden process and decay is a fuzzy concept. Decay is non-linear and hard to characterize at the organism level. Sudden regime changes can happen over the course of days, see: menopause, post-viral syndromes, acute neuropathy, autoimmune diseases. To make things harder, we are evolutionarily and culturally adapted to obfuscate signs of decay. Defining decay at the molecular level is presently an intractable information problem. Theoretically possible but practically unsolvable. The above is, to the best of my outlining abilities, the paradox most researchers get stuck in when they ponder aging. Is aging real? Common sense dictates aging happens, we see it all around us. But what data can we use to back up this intuition? This graph is the most clear explanation of aging and damning proof of its existence and effects. We are born, we are weak and unadapted to our environment and thus we die.

=== Science (4 posts) ===

• The Most Forbidden Technique
  By Zvi (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, AI Safety, Economics, Technology
  The Most Forbidden Technique is training an AI using interpretability techniques. An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that. You train on [X]. Only [X]. Never [M], never [T]. Why? Because [T] is how you figure out when the model is misbehaving. If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on. Those bits of optimization pressure from [T] are precious. Use them wisely. Table of Contents 1. New Paper Warns Against the Most Forbidden Technique. 2. Reward Hacking Is The Default. 3. Using CoT to Detect Reward Hacking Is Most Forbidden Technique. 4. Not Using the Most Forbidden Technique Is Harder Than It Looks. 5. It’s You, It’s Also the Incentives. 6. The Most Forbidden Technique Quickly Backfires. 7. Focus Only On What Matters. 8. Is There a Better Way? 9.

• You don't actually need a physical multiverse to explain anthropic fine-tuning.
  By Fraser (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, AI Safety
  The standard argument usually goes something like this: > Alice: there's a lot about the Earth that seems suspiciously fine-tuned for the evolution of complex life. > > Our orbit happens to lie in this fairly narrow band that allows for liquid water, and it seems other solvents don't work nearly as well in supporting biochemistry. > We've lucked into an ozone layer and a strong magnetic field, shielding us from cosmic rays and solar radiation. > Our moon is far larger than we'd expect for a planet our size - to the extent that it might be more correct to classify the Earth-Moon system as a double planet. It's plausible that the lunar-powered periodic cycle of tidal conditions could have been crucial as a sort of temperature input in the simulated annealing process of evolution - very simple local optima are less likely be stable and dominant in a dynamic environment. > We've been dealt a perfect hand of accessible elements - H, C, N, O, P, and S - and we're unique among the terrestrial planets in our active plate tectonics to recirculate and buffer stuff that would otherwise build up or deplete from the biosphere.

• Existing UDTs test the limits of Bayesianism (and consistency)
  By Cole Wyeth (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, AI Safety, Economics, Meta, Philosophy
  Epistemic status: Using UDT as a case study for the tools developed in my meta-theory of rationality sequence so far, which means all previous posts are prerequisites. This post is the result of conversations with many people at the CMU agent foundations conference, including particularly Daniel A. Herrmann, Ayden Mohensi, Scott Garrabrant, and Abram Demski. I am a bit of an outsider to the development of UDT and logical induction, though I've worked on pretty closely related things. I'd like to discuss the limits of consistency as an optimality standard for rational agents. A lot of fascinating discourse and useful techniques have been built around it, but I think that it can be in tension with learning at the extremes. Updateless decision theory (UDT) is one of those extremes; but in order to think about it properly, we need to start with its Bayesian roots. Because, appropriately enough for a sequence on the meta-theory of rationality, I want to psychoanalyze the invention/inventors of UDT. Hopefully, we'll then be in a position to ask what we think we know and how we think we know it in regards to updatelessness (also sometimes called priorism), the driving idea behind UDT.

• (Anti)Aging 101
  By George3d6 (LessWrong) - 2025-03-12
  Also appears in: AI Safety
  A quest to solve aging must start with careful consideration of what it is. Aging is a constant in nature, from archaea to elephants. Where life finds death as counterpart, aging is unopposed, yet frames our understanding of both. To solve aging is to prevent death and avoid decay. Death is a rather sudden process and decay is a fuzzy concept. Decay is non-linear and hard to characterize at the organism level. Sudden regime changes can happen over the course of days, see: menopause, post-viral syndromes, acute neuropathy, autoimmune diseases. To make things harder, we are evolutionarily and culturally adapted to obfuscate signs of decay. Defining decay at the molecular level is presently an intractable information problem. Theoretically possible but practically unsolvable. The above is, to the best of my outlining abilities, the paradox most researchers get stuck in when they ponder aging. Is aging real? Common sense dictates aging happens, we see it all around us. But what data can we use to back up this intuition? This graph is the most clear explanation of aging and damning proof of its existence and effects. We are born, we are weak and unadapted to our environment and thus we die.

=== Economics (3 posts) ===

• The Most Forbidden Technique
  By Zvi (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, AI Safety, Science, Technology
  The Most Forbidden Technique is training an AI using interpretability techniques. An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that. You train on [X]. Only [X]. Never [M], never [T]. Why? Because [T] is how you figure out when the model is misbehaving. If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on. Those bits of optimization pressure from [T] are precious. Use them wisely. Table of Contents 1. New Paper Warns Against the Most Forbidden Technique. 2. Reward Hacking Is The Default. 3. Using CoT to Detect Reward Hacking Is Most Forbidden Technique. 4. Not Using the Most Forbidden Technique Is Harder Than It Looks. 5. It’s You, It’s Also the Incentives. 6. The Most Forbidden Technique Quickly Backfires. 7. Focus Only On What Matters. 8. Is There a Better Way? 9.

• Existing UDTs test the limits of Bayesianism (and consistency)
  By Cole Wyeth (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, AI Safety, Science, Meta, Philosophy
  Epistemic status: Using UDT as a case study for the tools developed in my meta-theory of rationality sequence so far, which means all previous posts are prerequisites. This post is the result of conversations with many people at the CMU agent foundations conference, including particularly Daniel A. Herrmann, Ayden Mohensi, Scott Garrabrant, and Abram Demski. I am a bit of an outsider to the development of UDT and logical induction, though I've worked on pretty closely related things. I'd like to discuss the limits of consistency as an optimality standard for rational agents. A lot of fascinating discourse and useful techniques have been built around it, but I think that it can be in tension with learning at the extremes. Updateless decision theory (UDT) is one of those extremes; but in order to think about it properly, we need to start with its Bayesian roots. Because, appropriately enough for a sequence on the meta-theory of rationality, I want to psychoanalyze the invention/inventors of UDT. Hopefully, we'll then be in a position to ask what we think we know and how we think we know it in regards to updatelessness (also sometimes called priorism), the driving idea behind UDT.

• The Social Economy
  By kylefurlong (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities
  The Social Economy is a proposal in political economy to equalize purchasing power through income based pricing. Informally called PMI, this new "currency" gives everyone about 100 PMI worth of purchasing power each month. Since the average monthly income in the United States is about 4000 dollars, 1 PMI roughly equates to about 40 dollars. Note, though, that a high income earner spending 0.25 PMI on a burrito will spend much more in dollars than someone with no income. This works for business because at the end of every month, the IRS (or whatever new body administers the PMI transactions) "evens" revenue. That is, businesses with higher than average PMI values per transaction transfer some of their base currency revenue to those with lower than average PMI values. This means both that businesses can count on a "stable" PMI value per transaction, and serve communities with lower incomes without affecting their revenue. PMI alone creates radical social change. When everyone has equal purchasing power, there is no undue economic leverage anyone can apply to anyone else. To make it work though, there need to be new monetary controls in the economy.

=== Meta (2 posts) ===

• Field Testing in Brazilian Almost Military Training
  By P. João (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities
  Publishing stories like this makes me feel more naked than if I were doing a striptease. And the worst part is that no one puts dollars in my underwear, but here it goes: When I decided to write for the LessWrong community about the mechanisms of humor, I thought: "If I managed to avoid jail by using acid comedy in military barracks, I can certainly contribute something useful here. And here's an application." Pre-Haiti Context I received the most Brazilian mission possible: Teach emergency first aid to an entire regiment of the Brazilian army "So it's for a peace mission? Will I be able to do this alone?" "Relax, soldier, you have a week to train them" The standard is 3 intensive months. My solution? Condense intensive care to "priority life" principles: E.g., Broken spine vs. Cardiac arrest - What good is a spine to someone who isn't breathing? And highlight this wisdom with a principle even soldiers couldn't forget: "What's a fart to someone who's already crapped themselves?" Day 1 - The Bathroom Revolution After the fart principle, the class turned into a minefield.

• Existing UDTs test the limits of Bayesianism (and consistency)
  By Cole Wyeth (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, AI Safety, Science, Economics, Philosophy
  Epistemic status: Using UDT as a case study for the tools developed in my meta-theory of rationality sequence so far, which means all previous posts are prerequisites. This post is the result of conversations with many people at the CMU agent foundations conference, including particularly Daniel A. Herrmann, Ayden Mohensi, Scott Garrabrant, and Abram Demski. I am a bit of an outsider to the development of UDT and logical induction, though I've worked on pretty closely related things. I'd like to discuss the limits of consistency as an optimality standard for rational agents. A lot of fascinating discourse and useful techniques have been built around it, but I think that it can be in tension with learning at the extremes. Updateless decision theory (UDT) is one of those extremes; but in order to think about it properly, we need to start with its Bayesian roots. Because, appropriately enough for a sequence on the meta-theory of rationality, I want to psychoanalyze the invention/inventors of UDT. Hopefully, we'll then be in a position to ask what we think we know and how we think we know it in regards to updatelessness (also sometimes called priorism), the driving idea behind UDT.

=== Other (1 posts) ===

• Who is your favorite person, and why?
  By tailcalled (LessWrong) - 2025-03-12
  Discuss.

=== Technology (1 posts) ===

• The Most Forbidden Technique
  By Zvi (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, AI Safety, Science, Economics
  The Most Forbidden Technique is training an AI using interpretability techniques. An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that. You train on [X]. Only [X]. Never [M], never [T]. Why? Because [T] is how you figure out when the model is misbehaving. If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on. Those bits of optimization pressure from [T] are precious. Use them wisely. Table of Contents 1. New Paper Warns Against the Most Forbidden Technique. 2. Reward Hacking Is The Default. 3. Using CoT to Detect Reward Hacking Is Most Forbidden Technique. 4. Not Using the Most Forbidden Technique Is Harder Than It Looks. 5. It’s You, It’s Also the Incentives. 6. The Most Forbidden Technique Quickly Backfires. 7. Focus Only On What Matters. 8. Is There a Better Way? 9.

=== Philosophy (1 posts) ===

• Existing UDTs test the limits of Bayesianism (and consistency)
  By Cole Wyeth (LessWrong) - 2025-03-12
  Also appears in: AI Capabilities, AI Safety, Science, Economics, Meta
  Epistemic status: Using UDT as a case study for the tools developed in my meta-theory of rationality sequence so far, which means all previous posts are prerequisites. This post is the result of conversations with many people at the CMU agent foundations conference, including particularly Daniel A. Herrmann, Ayden Mohensi, Scott Garrabrant, and Abram Demski. I am a bit of an outsider to the development of UDT and logical induction, though I've worked on pretty closely related things. I'd like to discuss the limits of consistency as an optimality standard for rational agents. A lot of fascinating discourse and useful techniques have been built around it, but I think that it can be in tension with learning at the extremes. Updateless decision theory (UDT) is one of those extremes; but in order to think about it properly, we need to start with its Bayesian roots. Because, appropriately enough for a sequence on the meta-theory of rationality, I want to psychoanalyze the invention/inventors of UDT. Hopefully, we'll then be in a position to ask what we think we know and how we think we know it in regards to updatelessness (also sometimes called priorism), the driving idea behind UDT.

